<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://optum.github.io/blog</id>
    <title>Optum Open Source Blog</title>
    <updated>2022-02-08T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://optum.github.io/blog"/>
    <subtitle>Optum Open Source Blog</subtitle>
    <icon>https://optum.github.io/img/open_source.svg</icon>
    <entry>
        <title type="html"><![CDATA[Migrating ETL to Spark - Refactoring]]></title>
        <id>/2022/02/08/spark-part2-refactoring</id>
        <link href="https://optum.github.io/blog/2022/02/08/spark-part2-refactoring"/>
        <updated>2022-02-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[_Note: this is the second article in a multi-part series. The first post covered]]></summary>
        <content type="html"><![CDATA[<p><em>Note: this is the second article in a multi-part series. The <a href="https://opensource.optum.com/blog/2022/01/14/spark-series-part-1" target="_blank" rel="noopener noreferrer">first post</a> covered
getting started by copy-pasting SQL into Spark with some rewrites. Future installments will cover topics like performance optimization and validation.</em></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="when-sql-isnt-enough">When SQL isn't enough<a class="hash-link" href="#when-sql-isnt-enough" title="Direct link to heading">​</a></h3><p>First, the term "Spark SQL" can be confusing. <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank" rel="noopener noreferrer">"Spark SQL"</a> refers to the structured data processing module within Spark. You can interact with Spark SQL through SQL SELECT queries, and through the DataFrame/Dataset API. For the rest of this article, when I say "SQL" I am referring to SQL syntax.</p><p>At Optum, we often start migrating database-backed processes to Spark by copy-pasting SQL at first, then refactor to use the DataFrame or Dataset API when it makes sense.</p><p>The two main scenarios where it is useful to refactor SQL-based code:</p><ul><li>DRY for transforming query results. You can use the power of Scala and functional programming to apply "for each column" logic.</li><li>Complex business rules that are hard to express in SQL. Spark can distribute computations that go beyond the SQL set operations, and a row in a dataframe can contain nested objects, not just values.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="stay-dry-with-the-dataframe-api">Stay DRY with the DataFrame API<a class="hash-link" href="#stay-dry-with-the-dataframe-api" title="Direct link to heading">​</a></h3><p>One simple example is, suppose your original SQL query from Oracle or SQL Server selected all the column names as upper-case and you want to ensure that the generated Parquet column names are all lower-case. Rather than trying to parse the SQL text and replace all the column names, you can apply this sort of logic on the DataFrame itself:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token plain">val result: DataFrame = spark.sql("SELECT ....")</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">result.schema.fieldNames.foldLeft(result) { (colName, df) =&gt; df.withColumnRenamed(colName, colName.toLowerCase()) }</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>There is an explanation of how <code>foldLeft</code> works on DataFrames <a href="https://stackoverflow.com/a/52028279/836318" target="_blank" rel="noopener noreferrer">on this StackOverflow post</a>. The above code is looping over the column names on the original DataFrame, and for each column, returns a new DataFrame that renames the original column to the lower-case name. The functional syntax abstracts away reassignment of the DataFrame for each column, keeping all variables immutable.</p><p>You can use the DataFrame API to replace SQL syntax altogether, which is especially useful when you have repeating patterns.</p><p>For example if you have a query like this</p><div class="codeBlockContainer_I0IT language-sql theme-code-block"><div class="codeBlockContent_wNvx sql"><pre tabindex="0" class="prism-code language-sql codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">SELECT</span><span class="token plain"> cost1 </span><span class="token operator" style="color:rgb(137, 221, 255)">*</span><span class="token plain"> factor_val</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> cost2 </span><span class="token operator" style="color:rgb(137, 221, 255)">*</span><span class="token plain"> factor_val</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> cost3 </span><span class="token operator" style="color:rgb(137, 221, 255)">*</span><span class="token plain"> factor_val</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">FROM</span><span class="token plain"> costs</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">JOIN</span><span class="token plain"> lookup_factor </span><span class="token keyword" style="font-style:italic">on</span><span class="token plain"> costs</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">factor_id </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> lookup_factor</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">factor_id</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Where you have a series of identical, numbered columns all multiplied by some factor from a lookup table.</p><p>This can be rewritten as follows:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token plain">val costColumnRange = (1 to 10)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">val joined = spark.table("costs")</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  .join(spark.table("lookup_factor"), Seq("factor_id"))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">val costColumnsMultiplied = costColumnRange.map(i =&gt; col(s"cost$i") * $"factor")</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">val final = joined.select(costColumnsMultiplied)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Here, we start with a DataFrame that represents the two tables, <code>costs</code> and <code>lookup_factor</code>, joined. No columns are specified at this point, so the <code>joined</code> DataFrame has all columns from the original tables available. This is equivalent to <code>select * from costs join lookup_factor</code>.</p><p>The <code>map</code> transforms the range of column suffixes in <code>costColumnRange</code> to a list of Spark <code>Column</code> objects, where each Column represents a single "costX <!-- -->*<!-- --> factor" column that would appear in a <code>SELECT</code> statmeent.</p><p>The final result is produced by passing the list of <code>Column</code> objects to <code>select</code>.</p><p>This construct is useful for patterns that repeat with similar column names or column names with numeric indexes.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="get-complex-business-logic-out-of-sql">Get complex business logic out of SQL<a class="hash-link" href="#get-complex-business-logic-out-of-sql" title="Direct link to heading">​</a></h3><p>Sometimes complex analytics are calculated in SQL. For example, clinical quality measures typically follow a form like:</p><ul><li>Patient qualifies for inclusion in metric denominator, by looking at procedure codes for an eligible visit in some time range</li><li>Patient is not excluded from the calculation for some reason; for example, if a patient can't get a flu shot because of an allergy, that patient is not included counted in the deonominator</li><li>Patient satisfies the metric numerator; for example, the patient got their flu shot.</li></ul><p>In SQL, a typical implementation is building temp tables for each measure. In this approach, each SQL query represents one component calculated for all patients:</p><div class="codeBlockContainer_I0IT language-sql theme-code-block"><div class="codeBlockContent_wNvx sql"><pre tabindex="0" class="prism-code language-sql codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">create</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">table</span><span class="token plain"> eligible_patients </span><span class="token keyword" style="font-style:italic">as</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">select</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">distinct</span><span class="token plain"> patient_id</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> patient</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">join</span><span class="token plain"> patient_procedure </span><span class="token keyword" style="font-style:italic">using</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">patient_id</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">where</span><span class="token plain"> procedure_code </span><span class="token operator" style="color:rgb(137, 221, 255)">in</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">codes </span><span class="token keyword" style="font-style:italic">for</span><span class="token plain"> eligible visit</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token operator" style="color:rgb(137, 221, 255)">and</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">not</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">exists</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token keyword" style="font-style:italic">select</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">   </span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> patient_diagnosis</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">   </span><span class="token keyword" style="font-style:italic">where</span><span class="token plain"> patient_diagnosis</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">patient_id </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> patient_procedure</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">patient_id</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">   </span><span class="token operator" style="color:rgb(137, 221, 255)">and</span><span class="token plain"> diagnosis_code </span><span class="token operator" style="color:rgb(137, 221, 255)">in</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">codes that indicate exclusion</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">create</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">table</span><span class="token plain"> patient_numerators </span><span class="token keyword" style="font-style:italic">as</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">select</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">distinct</span><span class="token plain"> patient_id</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> patients</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">join</span><span class="token plain"> procedures </span><span class="token keyword" style="font-style:italic">using</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">patient_id</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">where</span><span class="token plain"> procedure_code </span><span class="token operator" style="color:rgb(137, 221, 255)">in</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">codes that indicate numerator satisfied</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">create</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">table</span><span class="token plain"> measure </span><span class="token keyword" style="font-style:italic">as</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">select</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">count</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">numerator</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">patient_id</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token operator" style="color:rgb(137, 221, 255)">/</span><span class="token function" style="color:rgb(130, 170, 255)">count</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">denominator</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">patient_id</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">as</span><span class="token plain"> measure</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> eligible_patients denominator</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">left</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">join</span><span class="token plain"> patient_numerators </span><span class="token keyword" style="font-style:italic">as</span><span class="token plain"> numerator</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>This is workable for one measure, but what happens when you need to calculate dozens of such measures? This kind of logic usually ends up requiring separate scans on tables like <code>patient_diagnosis</code> or <code>patient_procedure</code> for each measure, whether a separate temp table or a separate <code>[not] exists</code> subquery. So each scan ends up calcuating one measure or measure precursor for all patients at once.</p><p>In Spark, you are not limited to SQL set operations. You can treat rows in DataFrames like objects with nested collections (lists or sets), and use any procedural or functional logic on these objects. Then, you can take a completely different approach:</p><ul><li>fetch the all the related records for a patient at once, as a single Patient object</li><li>for each patient, calculate all the measures for the patient in memory.</li></ul><p>The first step in this approach is to retrieve the data into an object graph so it's easier to work with:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token plain">case class Diagnosis(diagnosis_code: String, ...)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">case class Procedure(procedure_code: String, ...)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">case class Patient(patient_id: Long, dx: Seq[Diagnosis], px: Seq[Procedure])</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">val groupedDx = spark.table("patient_diagnosis")</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  .groupBy("patient_id")</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  .agg(collect_list(struct("diagnosis_code", ... ).as("diagnosis_list")))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">val groupedPx = spark.table("patient_procedure")</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  .groupBy("patient_id")</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  .agg(collect_list(struct("procedure_code", ... ).as("procedure_list")))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">val patients = spark.table("patient")</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  .join(groupedDx, Seq("patient_id"))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  .join(groupedPx, Seq("patient_id"))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  .as[Patient]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>This code aggregates the records from <code>patient_procedure</code> and <code>patient_diagnosis</code> tables into temporary DataFrames with a single row for each patient. Each row has two fields: <code>patient_id</code>, and an embedded list of procedure or diagnosis codes.</p><p>Since each of these temporary DataFrames only contains one record per patient, they can be joined safely. We could never do a join like this in SQL because a join between <code>patient_procedure</code> and <code>patient_diagnosis</code> would be similar to a cartesian join (more precisely, a cartesian join within single patient, for the procedure and diagnosis codes that match our criteria).</p><p>The final call <code>.as[Patient]</code> turns the Spark DataFrame into a typed Dataset, where each row is a <code>Patient</code> <em>object</em>, defined by the case classes at the beginning. Fields and collections in these objects may then be accessed or iterated, as if they were plain Scala objects.</p><p>This code will take that set of Patients, and call <code>calculateQualityMeasures</code> on each one individually (in parallel, with different partitions of patients on different Spark executors), resulting in a Dataset of <code>QualityMeasure</code> objects:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token plain">case class QualityMeasure(eligible1: Boolean, satisfies1: Boolean, eligible2: Boolean, satisfies2: Boolean, ...)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">def calculateQualityMeasures(p: Patient): QualityMeasure = {</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  val eligible1 = !p.px.intersect(ELIGIBLE_VISIT_PX_CODES_1).empty &amp;&amp; p.dx.intersect(EXCLUDE_VISIT_DX_CODES_1).empty</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  val satisfies1 = !p.px.intersect(PX_CODES_1).empty</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  val eligible2 = !p.px.intersect(ELIGIBLE_VISIT_PX_CODES_2).empty &amp;&amp; p.dx.intersect(EXCLUDE_VISIT_DX_CODES_2).empty</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  val satisfies2 = !p.px.intersect(PX_CODES_2).empty</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  QualityMeasure(eligible1, satisfies1, eligible2, satisfies2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">val results = patients.map(calculateQualityMeasures)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>The <code>map</code> method on the Dataset acts like <code>map</code> on any other Scala collection, where <code>calculateQualityMeasures</code> is a pure function that takes a Patient in and returns QualityMeasures out.</p><p>Note that <code>calculateQualityMeasures</code> has no Spark dependencies and can be unit-tested on Scala case class instances independently!</p><p>Because the Patient object has the full collection of procedure and diagnosis codes in memory, we can perform multiple iterations over the same procedure and diagnosis codes for the patient to calculate all quality measures at the same time. This is a big difference from SQL, where logic operations like <code>where exists</code> are coupled to storage operations to fetch data from disk.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="coming-up">Coming up<a class="hash-link" href="#coming-up" title="Direct link to heading">​</a></h3><p>The next article will discuss testing and validation strategies.</p>]]></content>
        <author>
            <name>Bill Schneider</name>
            <uri>https://www.linkedin.com/in/wrschneider</uri>
        </author>
        <category label="Spark" term="Spark"/>
        <category label="ETL" term="ETL"/>
        <category label="Engineering" term="Engineering"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Streaming At Scale With Benthos]]></title>
        <id>/2022/01/25/data-streaming-at-scale-with-benthos</id>
        <link href="https://optum.github.io/blog/2022/01/25/data-streaming-at-scale-with-benthos"/>
        <updated>2022-01-25T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Data Streaming At Scale With Benthos]]></summary>
        <content type="html"><![CDATA[<h3 class="anchor anchorWithStickyNavbar_mojV" id="data-streaming-at-scale-with-benthos">Data Streaming At Scale With Benthos<a class="hash-link" href="#data-streaming-at-scale-with-benthos" title="Direct link to heading">​</a></h3><p>Optum's <a href="https://opensource.optum.com/blog/2021/08/24/OSPO%20culture" target="_blank" rel="noopener noreferrer">Open Source Program Office (OSPO)</a> recently launched an Open Source sponsorship program, committing an initial $50K total donations to external Open Source projects of notable merit. <a href="https://benthos.dev/" target="_blank" rel="noopener noreferrer">Benthos</a>, the stream processor for mundane tasks, was selected as the first project to receive a donation through this program.</p><p>This article is the first part of a longer series of posts about Benthos, where I'll share some insight into why we decided to adopt Benthos as the core data streaming engine for Optum's new <a href="https://martinfowler.com/articles/data-mesh-principles.html" target="_blank" rel="noopener noreferrer">Data Mesh</a> solution.</p><p><img alt="Benthos logo" src="/assets/images/benthos.dev-2441cb2ce43bf8abd226cbb6412b455e.svg" width="640" height="400"></p><p>Benthos is a stateless data streaming engine written entirely in <a href="https://go.dev/" target="_blank" rel="noopener noreferrer">Go</a> and compiled as a single static binary. It enables data engineers to configure a pipeline of processing steps that are to be executed on messages as they flow from the source to the sink during runtime. Through back pressure mechanisms, it ensures at-least-once delivery when connecting to sources and sinks which support this, without persisting in-flight messages in any temporary storage. It has a wide range of <a href="https://www.benthos.dev/docs/about/#components" target="_blank" rel="noopener noreferrer">builtin connectors</a> that are expanding constantly and it's very easy to <a href="https://www.benthos.dev/docs/guides/getting_started" target="_blank" rel="noopener noreferrer">deploy</a> in production. This project was started over 6 years ago by <a href="https://www.jeffail.uk/" target="_blank" rel="noopener noreferrer">Ashley Jeffs</a> (he goes by Ash), who is still maintaining it and is currently <a href="https://github.com/Jeffail/benthos/issues/1071" target="_blank" rel="noopener noreferrer">preparing</a> for the 4th major release.</p><p>At Optum, we have large volumes of disperse data in various formats spread across many different datastores, some of which are legacy systems that are slowly getting phased out. Benthos allows us to perform data movement from these systems to modern cloud-based datastores, where data analysts can perform their work more efficiently. While Benthos doesn't provide connectors for all the types of sources and sinks that we need, it does come with a rich <a href="https://pkg.go.dev/github.com/Jeffail/benthos/v3/public/service" target="_blank" rel="noopener noreferrer">Golang API</a> for writing custom plugins that allow us to expand its functionality and tailor it for our needs.</p><p>Since Benthos can easily be run in Kubernetes clusters, it enables us to build robust and efficient systems for terabyte-size workloads which scale horizontally. In order to monitor our production workloads, we leveraged the various Prometheus-compatible <a href="https://www.benthos.dev/docs/components/metrics/about" target="_blank" rel="noopener noreferrer">metrics</a> and Jaeger-compatible <a href="https://www.benthos.dev/docs/components/tracers/about" target="_blank" rel="noopener noreferrer">traces</a> that Benthos emits for each component of its configured processing pipeline. For complex workflows, it can be configured to add extra metrics using the <a href="https://www.benthos.dev/docs/components/processors/metric" target="_blank" rel="noopener noreferrer"><code>metric</code> processor</a>.</p><p>Additionally, we use distributed event streaming platforms, such as <a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka</a>, as message buses for various realtime and batch ETL (Extract, Transform, Load) workflows. Benthos comes with an embedded DSL called <a href="https://www.benthos.dev/docs/guides/bloblang/about" target="_blank" rel="noopener noreferrer">Bloblang</a>, which lets us express complex transformations of structured data, such as schema migrations and validations.</p><p>During our Benthos adoption, we pushed upstream several new adaptors and features that were quickly approved and merged. Getting code merged into Benthos has been one of my best experiences in contributing to Open Source so far.</p><p><a href="https://studio.benthos.dev/" target="_blank" rel="noopener noreferrer">Benthos Studio</a> has been released recently to aid users in designing, visualising and testing Benthos streaming pipelines and it's proving very valuable when building new pipelines from scratch or navigating complex ones.</p><p>If you'd like to learn more about Benthos, Ash's <a href="https://www.youtube.com/c/Jeffail" target="_blank" rel="noopener noreferrer">YouTube channel</a> is a good place to start. Also, there is a vibrant <a href="https://www.benthos.dev/community" target="_blank" rel="noopener noreferrer">community</a> around it, where Ash and other users like myself are available to answer any questions that you might have.</p><p>Try <a href="https://benthos.dev/" target="_blank" rel="noopener noreferrer">Benthos</a> today if you haven't already!</p>]]></content>
        <author>
            <name>Mihai Todor</name>
            <uri>https://www.linkedin.com/in/mtodor</uri>
        </author>
        <category label="Benthos" term="Benthos"/>
        <category label="Golang" term="Golang"/>
        <category label="Kafka" term="Kafka"/>
        <category label="Data Streaming" term="Data Streaming"/>
        <category label="ETL" term="ETL"/>
        <category label="Engineering" term="Engineering"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Migrating ETL to Spark - Motivation and Quick Start]]></title>
        <id>/2022/01/14/spark-series-part-1</id>
        <link href="https://optum.github.io/blog/2022/01/14/spark-series-part-1"/>
        <updated>2022-01-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Note: this is the first article in a multi-part series. Future installments will cover topics like performance optimization, validation, and refactoring.]]></summary>
        <content type="html"><![CDATA[<p><em>Note: this is the first article in a multi-part series. Future installments will cover topics like performance optimization, validation, and refactoring.</em></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="advantages-of-spark">Advantages of Spark<a class="hash-link" href="#advantages-of-spark" title="Direct link to heading">​</a></h3><p>At Optum, we are migrating some of our ETL processes to <a href="https://spark.apache.org" target="_blank" rel="noopener noreferrer">Apache Spark</a>, an open-source framework for distributed computing. Many of these processes were previously running on relational databases, based on SQL queries and stored procedures. The data transformations can range from simple cleansing, to table joins and aggregations, to complex business logic. The results are then stored in a data warehouse for use in analytics applications.</p><p>While DBs have their advatanges, there's a limit to how big of a dataset you can process in a given timeframe. That's because databases are generally meant to scale vertically -- when you have a bigger dataset, you get a bigger server. At some point, though, you can't make your server any bigger. Spark, like other big-data platforms, is meant to scale horizontally -- when you get a bigger dataset, you add <em>more</em> servers.</p><p>Horizontal scaling is especially useful when you run Spark in the public cloud, given the options for on-demand and serverless compute. Cloud gives the flexibility to scale up resources to meet demand rather than queueing jobs until there is capacity on a fixed server or Spark cluster. Then you pay for your compute resources by the CPU-hour, rather than having to size your cluster for the peak.</p><p>Spark lets you build your data transformations with SQL syntax or SQL-like constructs, so there is a natural transition from a SQL-based ETL process to Spark. Rather than processing joins and aggregations on a single server, though, Spark datasets are distributed. Each node in a cluster works on a partition of the data in parallel. Intermediate results from one <code>JOIN</code> or <code>GROUP BY</code> are then redistributed, or <em>shuffled</em>, to different nodes for the next operation.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="getting-started-with-spark-sql">Getting started with Spark SQL<a class="hash-link" href="#getting-started-with-spark-sql" title="Direct link to heading">​</a></h3><p><em>For this and future sections, I assume you have some familiarity with Spark and how to author and run Spark jobs. There are plenty of tutorials out there, including some good resources in the <a href="https://spark.apache.org/docs/latest/sql-getting-started.html" target="_blank" rel="noopener noreferrer">Spark documentation</a></em></p><p>Spark offers SQL syntax, but Spark is not a database! Spark datasets are in-memory, and are explicitly read or written from storage. The separation of storage and compute helps with horizontal scale as both can be scaled independently. Typically if our job is running in the cloud, data will be stored in object/blob storage.</p><p>A typical ETL process might have something like this:</p><div class="codeBlockContainer_I0IT language-sql theme-code-block"><div class="codeBlockContent_wNvx sql"><pre tabindex="0" class="prism-code language-sql codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">TABLE</span><span class="token plain"> patient_visits</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">patient_id </span><span class="token keyword" style="font-style:italic">bigint</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> patient_name </span><span class="token keyword" style="font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token number" style="color:rgb(247, 140, 108)">100</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> visit_count </span><span class="token keyword" style="font-style:italic">int</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">INTO</span><span class="token plain"> patient_visits</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">SELECT</span><span class="token plain"> patient_id</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> patient_name</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">count</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"> visit_count</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">FROM</span><span class="token plain">  patient</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">JOIN</span><span class="token plain">  visit </span><span class="token keyword" style="font-style:italic">ON</span><span class="token plain"> patient</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">patient_id </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> visit</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">patient_id</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">GROUP</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">BY</span><span class="token plain"> patient_id</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> patient_name</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In Spark, there is no <code>INSERT</code> statement, only <code>SELECT</code>. When you run the SQL <code>SELECT</code> query in Spark SQL, you would get back a DataFrame, which is a tabular abstraction on top of a Spark Resilient Distributed Dataset (RDD). You would then write the DataFrame explicitly to storage:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token plain">val df = spark.sql("""</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  SELECT patient_id, patient_name, count(1) visit_count</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  FROM  patient</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  JOIN  visit ON patient.patient_id = visit.patient_id</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  GROUP BY patient_id, patient_name</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">""")</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">val targetLocation = // s3 or Azure blob store path</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">df.write.parquet(targetLocation)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>The output format is typically <a href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html" target="_blank" rel="noopener noreferrer">Parquet</a>, which uses compressed and column-oriented storage. That means if you run a query that only reads two columns, you only retrieve the data for those two columns from storage. Most database tables use row-oriented storage by default which means you read all the columns from storage unless you can use a covering index. The Parquet format also contains schema information, like a database table. So unlike text formats, there is a distinction between <code>3</code> (integer), <code>3.000....</code> (floating point) and <code>3.00</code> (fixed-point, two decimal places).</p><p>Many of your SQL SELECT statements, from SQL Server/Oracle/etc. can often be copy-paste (minus the <code>INSERT</code>) and run with minimal modification to address vendor-specific differences for things like string concatenation (SQL Server uses <code>+</code>), SQL <code>isnull</code> vs. Oracle <code>nvl</code> vs. <code>coalesce</code>, date functions etc.</p><p>But there are a few points of caution to look out for:</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="updates-and-deletes">UPDATEs and DELETEs<a class="hash-link" href="#updates-and-deletes" title="Direct link to heading">​</a></h3><p>Sometimes SQL-based ETL procesess use <code>UPDATE</code> or <code>DELETE</code> statements. Or you may have a series of <code>INSERT</code> statements a table with <code>WHERE NOT EXISTS</code> to prevent duplicates.</p><p>These cannot be copied as-is to Spark. Rather, you need to do some refactoring to work with <code>SELECT</code> (read) operations only. For example:</p><ul><li>Multiple <code>INSERT</code> statements become multiple <code>SELECT</code> statements combined with <code>UNION</code></li><li>An <code>UPDATE</code> becomes an outer join and combined in the original <code>SELECT</code></li><li>A <code>DELETE</code> becomes a filter against the original <code>SELECT</code></li><li>You might need to write the original <code>SELECT</code> results to a temporary location.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="correlated-subqueries">Correlated subqueries<a class="hash-link" href="#correlated-subqueries" title="Direct link to heading">​</a></h3><p>Some kinds of subqueries need to be re-written. For example, Spark does not support <code>EXISTS</code>/<code>NOT EXISTS</code> subqueries. These must be rewritten as <a href="https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html" target="_blank" rel="noopener noreferrer">semi-joins or anti-joins</a>. For example:</p><div class="codeBlockContainer_I0IT language-sql theme-code-block"><div class="codeBlockContent_wNvx sql"><pre tabindex="0" class="prism-code language-sql codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">select</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">*</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> table_a a </span><span class="token keyword" style="font-style:italic">where</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">not</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">exists</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token keyword" style="font-style:italic">select</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> table_b b </span><span class="token keyword" style="font-style:italic">where</span><span class="token plain"> b</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">a_id</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">a_id</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>becomes</p><div class="codeBlockContainer_I0IT language-sql theme-code-block"><div class="codeBlockContent_wNvx sql"><pre tabindex="0" class="prism-code language-sql codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">select</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">*</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> table_a a ANTI </span><span class="token keyword" style="font-style:italic">JOIN</span><span class="token plain"> table_b </span><span class="token keyword" style="font-style:italic">on</span><span class="token plain"> a</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">a_id </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> b</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">a_id</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="implicit-type-conversions">Implicit type conversions<a class="hash-link" href="#implicit-type-conversions" title="Direct link to heading">​</a></h3><p>In SQL-based ETL processes, an <code>INSERT</code> statement may do an implicit type conversion to the target column. In Spark you will have to do that conversion explicitly.</p><p>At the same time, Spark may do some implicit type conversion of its own on arithmetic and aggregate functions. For example, a <code>SUM</code> in Spark will promote the underlying numeric type: an <code>int</code> will become a <code>bigint</code>, a <code>decimal(p, s)</code> will become <code>decimal(p + 10, s)</code> (expand by ten digits). So if you intend to preserve the datatypes that correspond to your original database schema, you will have to cast explicitly.</p><p>For example:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token plain">val df = spark.sql("""</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  SELECT patient_id, patient_name, cast(count(1) as integer) visit_count</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  FROM  patient</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  JOIN  visit ON patient.patient_id = visit.patient_id</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  GROUP BY patient_id, patient_name</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">""")</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="decimal-truncation-on-division">Decimal truncation on division<a class="hash-link" href="#decimal-truncation-on-division" title="Direct link to heading">​</a></h3><p>On some databases (SQL Server for example), division operations on decimal types may truncate to a <a href="https://docs.microsoft.com/en-us/sql/t-sql/data-types/precision-scale-and-length-transact-sql?view=sql-server-ver15" target="_blank" rel="noopener noreferrer">specified number of decimal places</a>. Spark follows <a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DecimalPrecision.scala" target="_blank" rel="noopener noreferrer">similar rules to SQL Server</a> but rounds rather than truncates.</p><p>If your goal is to reproduce exact results in Spark (useful to simplify validation in a technology migration), you may need to define a <a href="https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html" target="_blank" rel="noopener noreferrer">user-defined function</a>:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token plain">val divide = udf((x: BigDecimal, y: BigDecimal, scale: Int) =&gt; x.divide(y, scale, RoundingMode.FLOOR))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">spark.udf.register("divide", divide)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">spark.sql("select divide(foo, bar, 6) as ratio from table")</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="coming-up">Coming up<a class="hash-link" href="#coming-up" title="Direct link to heading">​</a></h3><p>The next article will discuss going beyond copy-paste SQL, to refactoring and using the Spark DataFrame API directly.</p>]]></content>
        <author>
            <name>Bill Schneider</name>
            <uri>https://www.linkedin.com/in/wrschneider</uri>
        </author>
        <category label="Spark" term="Spark"/>
        <category label="ETL" term="ETL"/>
        <category label="Engineering" term="Engineering"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Packaging models and analytics for reuse -  API vs. Inner Source.]]></title>
        <id>/2021/11/15/package-model-analytic-reuse</id>
        <link href="https://optum.github.io/blog/2021/11/15/package-model-analytic-reuse"/>
        <updated>2021-11-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Packaging models and analytics for reuse - API vs. Inner Source.]]></summary>
        <content type="html"><![CDATA[<h3 class="anchor anchorWithStickyNavbar_mojV" id="packaging-models-and-analytics-for-reuse---api-vs-inner-source">Packaging models and analytics for reuse - API vs. Inner Source.<a class="hash-link" href="#packaging-models-and-analytics-for-reuse---api-vs-inner-source" title="Direct link to heading">​</a></h3><p>Various teams in Optum produce models or analytics. Simplified examples of "models":</p><ul><li>risk scoring for a patient - given history for a patient, how likely are they to be admitted to the hospital as an inpatient?</li><li>grouping claims into an episode - what is the all-in cost for a procedure including follow-up care?</li><li>calculating quality measures - in a patient population, what percentage got their flu shots this year?</li></ul><p>Models can be either rules-based and explicitly coded, or ML-based.</p><p>Once we have models like these, how can that IP be reused in a different context? The goal should be to be able to
compose various individual models or analytics into new products.</p><p>There are a few different approaches typically used:</p><p><strong>REST APIs</strong> The team that creates a model can deploy their model as a REST API for other teams to invoke. This
analytic-as-a-service approach
works well for APIs that are called inside of a workflow--for example, making a decision about an individual patient
at point of care, or analytics that can be recomputed incrementally without full history.</p><p>The benefit of APIs is that the provider and consumer tech stack doesn't need to align, and that consumers will see changes to the model immediately when it is deployed with no action required. The big downside is latency: REST doesn't work well in batch workflows if processing each individual record requires an HTTPS round-trip. The other downside is operational coupling: the team that produces the model needs to not only maintain the logic of the model itself, but also is responsible for availability and performance for the REST API. Also the provider can't make breaking changes without an API versioning strategy, keeping the old versions of the API live until consumers can migrate.</p><p><strong>Batch API</strong> A variation of the analytic-as-a-service approach to address the HTTPS round-trip issue with batch data, for analytics that involve aggregating historical data and can't recompute on incremental data alone. The payload goes through a file transfer (SFTP, Azure blob storage, AWS S3 etc.) and REST endpoints are only used to mediate the process with pointers
to the out-of-band payload (similar to
<a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/claim-check" target="_blank" rel="noopener noreferrer">claim check pattern</a>). Still, you have to ship
data back and forth and the file transfer time can add significant overhead to overall turnaround time. Also, you still
have operational coupling and concerns over breaking changes, although this could beinsulated from the end user interactions.</p><p><strong>Inner source</strong> Instead of offering analytics as a service, you can package an analytic as an artifact that can be
embedded as a dependency into any other application. This could either be a JAR or DLL that gets included as a build
dependency, or the output of an ML framework (PyTorch, Tensorflow etc.)</p><p>The benefits here are: you bring the analytic to the data, so there is no
overhead in transferring data back and forth. The model provider has no operational responsibility or cost. Consumers
can continue using old versions of artifacts until they are ready to upgrade. The downsides are: consumers have to
take action to upgrade. There are tech stack dependencies -- if the analytic is a JAR and your team is standardized on
.NET, or the ML framework doesn't have an SDK for your platform, you can't call it in-process. Tech stack alignment is
less of an issue for batch process, since different stages of a batch workflow can be on different technologies as long as
they share a file system.</p><p>There is no one right answer, and we use different approaches in different situations. To make a decision you have to ask yourself a series of questions and decide how important each one is:</p><ul><li>How often does the underlying model/analytic change?</li><li>As a consumer, do you want to see changes right away or do you want to be in control over moving to the new version?</li><li>Is this a real-time interaction or a batch process?</li><li>How big are concerns over data transfer overhead or request latency?</li><li>If you need a real-time interaction, are you on compatible tech stacks?</li></ul><p>Also note that <em>these approaches are not mutually exclusive.</em> If you are a team providing a model or analytic, and you
think about how to package your model for reuse via inner-source, you can <em>also</em> provide that same model as a REST API endpoint.</p>]]></content>
        <author>
            <name>Bill Schneider</name>
            <uri>https://www.linkedin.com/in/wrschneider</uri>
        </author>
        <category label="ML-AI" term="ML-AI"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Copying data between AWS and Azure]]></title>
        <id>/2021/10/21/s3-to-azure-copy</id>
        <link href="https://optum.github.io/blog/2021/10/21/s3-to-azure-copy"/>
        <updated>2021-10-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Copying data between AWS and Azure]]></summary>
        <content type="html"><![CDATA[<h3 class="anchor anchorWithStickyNavbar_mojV" id="copying-data-between-aws-and-azure">Copying data between AWS and Azure<a class="hash-link" href="#copying-data-between-aws-and-azure" title="Direct link to heading">​</a></h3><p>Many Optum teams are working in Azure, although some are also working in AWS.  So sometimes we have to transfer files between
AWS and Azure, for batch-processing pipelines that have components in both clouds.</p><p>The easiest way to move files between clouds is through their respective object storage APIs: Azure Blob Storage or
AWS S3.  Transfers are over port 443 with public-facing endpoints, and authentication/authorization is handled like
all other native cloud services.</p><p>There are good open-source tools for AWS/Azure file transfers, including
<a href="https://github.com/Azure/azure-storage-azcopy" target="_blank" rel="noopener noreferrer"><code>azcopy</code></a> and
<a href="https://github.com/rclone/rclone" target="_blank" rel="noopener noreferrer"><code>rclone</code></a>.  Both are command-line tools that can be executed from automated
processes.  <code>azcopy</code> is specialized to work with Azure, but lacks the ability to move data from Azure to
AWS S3.  <code>rclone</code> is general purpose any-to-any transfers, but may not be as optimized for uploads to Azure.</p><p>These tools can easily be installed in Docker containers.  If you are running on AWS, you might want to execute
a transfer to/from Azure as a serverless ECS Fargate task.  When you run the task, you would pass the specific
command line parameters in the <code>containerOverrides</code>.  This is all standard; the only tricky piece is
how to manage credentials for both AWS and Azure.  Secrets that can be passed through environment variables can be
<a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html" target="_blank" rel="noopener noreferrer">stored in AWS Parameter Store or Secrets Manager and retrieved with <code>secrets</code> in the container definition</a>.  Others require wrapper scripts to perform
additional tasks before running azcopy or rclone.</p><ul><li><p>azcopy </p><ul><li>AWS credentials: for some reason, <a href="https://github.com/Azure/azure-storage-azcopy/issues/1341" target="_blank" rel="noopener noreferrer">azcopy doesn't use native AWS profiles</a> and only takes AWS credentials from environment variables.  So you need a wrapper script to set these up.</li><li>Azure SAS tokens: azcopy expects these on the command line, so it's a good idea for a wrapper script to append this to the URL</li><li>Azure service principal: secret can be passed through <code>AZCOPY_SPA_CLIENT_SECRET</code> environment variable.</li></ul></li><li><p>Rclone </p><ul><li>AWS credentials: uses the AWS SDK and will pick up AWS credentials from the container's execution role.</li><li>Azure SAS tokens: SAS URL can be passed via <code>RCLONE_AZUREBLOB_SAS_URL</code> environment variable as container secret.</li><li>Azure service principal: needs to be stored as JSON in a file.  Your container needs a wrapper script that takes
the actual secret from the injected environment variable, and writes it to a file.</li></ul></li></ul><p>Examples of wrapper script for <code>azcopy</code> to set up AWS credentials from the ECS metadata endpoint:</p><div class="codeBlockContainer_I0IT language-sh theme-code-block"><div class="codeBlockContent_wNvx sh"><pre tabindex="0" class="prism-code language-sh codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token plain">response=$(wget -O - 169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">export AWS_ACCESS_KEY_ID=$(echo "$response" | jq -r '.AccessKeyId')</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">export AWS_SECRET_ACCESS_KEY=$(echo "$response" | jq -r '.SecretAccessKey')</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">export AWS_SESSION_TOKEN=$(echo "$response" | jq -r '.Token')</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">azcopy $@</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>If you are copying to Azure and need to keep the SAS token secret, you can append modify the above script to
append it to the URL and then omit the SAS token from the Azure blob storage URL passed in <code>containerOverrides</code>:</p><div class="codeBlockContainer_I0IT language-sh theme-code-block"><div class="codeBlockContent_wNvx sh"><pre tabindex="0" class="prism-code language-sh codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token plain">azcopy $@$AZURE_SECRET</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that this only works because azcopy can only be used to copy <em>from</em> S3 <em>to</em> Azure, not the other way around, so
we know the Azure URL will always be the final argument on the command line.</p><p>For rclone, this is a wrapper script to handle the service principal JSON:</p><div class="codeBlockContainer_I0IT language-sh theme-code-block"><div class="codeBlockContent_wNvx sh"><pre tabindex="0" class="prism-code language-sh codeBlock_jd64 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#bfc7d5"><span class="token plain"># assume AZURE_SECRET is configured with secret in container definition</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">echo "$AZURE_SECRET" &gt; /tmp/secret.json</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">export RCLONE_AZUREBLOB_SERVICE_PRINCIPAL_FILE=/tmp/secret.json</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">rclone $@</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>All of the above will allow you to do your Azure/AWS file transfers in a serverless Fargate task, that can be
integrated with whatever other pipeline workflow you have.</p>]]></content>
        <author>
            <name>Bill Schneider</name>
            <uri>https://github.com/wrschneider</uri>
        </author>
        <category label="AWS Azure S3 Cloud" term="AWS Azure S3 Cloud"/>
        <category label="engineering" term="engineering"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knative Channel Sans Kafka Admin Rights]]></title>
        <id>/2021/09/27/kn-kafka-topic-channel</id>
        <link href="https://optum.github.io/blog/2021/09/27/kn-kafka-topic-channel"/>
        <updated>2021-09-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Knative]]></summary>
        <content type="html"><![CDATA[<h3 class="anchor anchorWithStickyNavbar_mojV" id="knative">Knative<a class="hash-link" href="#knative" title="Direct link to heading">​</a></h3><p>Knative enables you to run a serverless platform on your own terms. It has 2 independent components, Serving and Eventing. Serving runs your application as a serverless container in a kubernetes cluster, while eventing provides the tooling to source and orchestate events to your application.</p><p>Once you get past the initial attraction of scale to 0 that serving provides, you will quickly take notice of knative eventing capabilities mentioned below</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="event-source">Event Source<a class="hash-link" href="#event-source" title="Direct link to heading">​</a></h3><p>Component provided by Knative Eventing to source events from a actual event producer like kafka/rabbit mq/github etc. and deliver it to a addressable resource (any resource which has a uri, can be knative service/kubernetes service or just an uri). There is an event source for almost all commonly used event producers. Either its community maintained or custom implementation.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="brokers-and-triggers">Brokers and Triggers<a class="hash-link" href="#brokers-and-triggers" title="Direct link to heading">​</a></h3><p>Source delivers events 1-1. A fan out model with filters would be great for orchestrating events. Thats what Brokers and Triggers provide. Broker as the name suggest is the event ingress and hub, triggers route the events based on filters.</p><p>For brokers, triggers and sources to work in harmony they need to speak the common language and that is provided by <a href="https://cloudevents.io" target="_blank" rel="noopener noreferrer">Cloud Events</a>.</p><p>Below image shows how the events are sourced and routed to different knative and kubernetes service.</p><p><img src="/assets/images/brokertrigger-b4491535bdff9e7c778019e6d1d035a5.jpg" width="955" height="442"></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="whats-a-knative-channel">What's a Knative Channel<a class="hash-link" href="#whats-a-knative-channel" title="Direct link to heading">​</a></h3><p>Brokers, as pointed above is the events hub, so it needs a state store, which is provided by <code>Channels</code>. By default Brokers use InMemory Channels. In a production environment, you would need a more robust store, for which Knative provides <code>KafkaChannel</code> and <code>NatsChannel</code>. Its worth mentioning here that there are broker implementations (kafka broker and rabbit mq broker) which dont require channels.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="kafka-topic-channel">Kafka Topic Channel<a class="hash-link" href="#kafka-topic-channel" title="Direct link to heading">​</a></h3><p>The kafka channel or kafka broker would need admin rights as topics are created for each resource. This would require the Knative operator to main a kafka cluster, which might be cumbersome. Hence we implemented a custom knative channel based on <code>kafka topic</code>.</p><p>Kafka Topic Channel or ktc conforms to the knative spec, hence brokers and triggers would work as is. It is a single tenant solution, each Broker resource would require separate kafka topic and subsequent kafka topic channel. The channel instance would be created in the user namespace.</p><p><a href="https://github.com/Optum/kafka-topic-channel" target="_blank" rel="noopener noreferrer">Details with examples</a></p><p>Bring your own kafka topic, get a knative channel!</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="related-resources">Related Resources<a class="hash-link" href="#related-resources" title="Direct link to heading">​</a></h4><ul><li><a href="https://www.youtube.com/watch?v=gXuW9mvj6xM&amp;list=PLnPNqTSUj2hKH5W7GWOZ-mzcw4r3O4bHj&amp;index=2&amp;t=14s" target="_blank" rel="noopener noreferrer">Kubecon demo on cloud events, knative brokers and triggers</a></li><li><a href="https://itsmurugappan.medium.com/writing-custom-knative-eventing-sources-92f6904131ad" target="_blank" rel="noopener noreferrer">Event Sources</a></li></ul>]]></content>
        <author>
            <name>Murugappan Chetty</name>
            <uri>https://github.com/itsmurugappan</uri>
        </author>
        <category label="engineering" term="engineering"/>
        <category label="knative" term="knative"/>
        <category label="channel" term="channel"/>
        <category label="broker" term="broker"/>
        <category label="kafka" term="kafka"/>
        <category label="message" term="message"/>
        <category label="events" term="events"/>
        <category label="cloudevents" term="cloudevents"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Racing League]]></title>
        <id>/2021/09/08/AI Racing League</id>
        <link href="https://optum.github.io/blog/2021/09/08/AI Racing League"/>
        <updated>2021-09-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[AI Racing League]]></summary>
        <content type="html"><![CDATA[<h3 class="anchor anchorWithStickyNavbar_mojV" id="ai-racing-league">AI Racing League<a class="hash-link" href="#ai-racing-league" title="Direct link to heading">​</a></h3><p>In March of 2019, I approached our executive leadership team in Optum Technology. We discussed the importance of making artificial intelligence (AI) a core part of the company culture. We brainstormed with stories of other companies' efforts. Then the conversation turned to work I had been doing outside of Optum teaching kids about AI. My partners and I have built coding clubs, such as CoderDojo Twin Cities. A lot of companies use an Open Source system called DonkeyCar to teach AI. We discussed how this might be a vehicle for our technical talent to contribute back by mentoring in communities.</p><p>These racing leagues are a great, innovative way to bring STEM mentorship to the next generation of healthcare leaders. The events are filled with fun and laughter. Clearly, these people and kids are having fun learning AI! Our Optum leaders were supportive - we agreed that Optum wants to invest in STEM mentorship. But how would we get the ball rolling to make this happen?</p><p>My leadership team guided me to write a proposal. Then they helped me identify a business sponsor in our internal training division. I also met people who were passionate about education and curriculum development, a topic I am also very interested in. We ran our first "AI Racing League" event on Aug. 6th 2019.</p><p>We learned a LOT about introducing AI/ML to non-technical staff during this first session. First of all, it was REALLY fun! We also learned how to break all the content and we designed a topic dependency graph of all the topics that we wanted to cover: RC car hardware, Nivida Nano hardware, calibration, cameras, UNIX shell, Python, data analysis, Jupyter Notebooks, computer vision, GPUs, training, model management, model evaluation.</p><p>For each of these concepts, we gathered documentation and resources and created breakout tables that our participants visited for the first part of the events. Then we broke down into teams and each team got a car they had to train. We built GPU servers that could build models in under five minutes.</p><p>The result? Many of our participants told us this was the “funnest” class they have EVER attended as part of our internal training. Since that event, we have had about a dozen follow-up events. As soon as we can gather again in person, we hope to continue live AI Racing League events. In the meantime, we're revising our materials with a focus on work on growth and multi-modal expansion. We are preparing for another great in-person racing season. We would love to collaborate with others that might find ways to host our curricula online and convert our materials for use primarily online. We now have many trained mentors and hundreds of alumni of the classes that are strongly recommending these courses to their colleagues. Reach out to me to connect!</p><p>We're happy to discuss how to bring this STEM mentorship program in ML/AI into your school or club!</p>]]></content>
        <author>
            <name>Dan McCreary</name>
            <uri>https://github.com/dmccreary</uri>
        </author>
        <category label="ML-AI" term="ML-AI"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Our UHG Open Source Program Office (OSPO)]]></title>
        <id>/2021/08/24/OSPO culture</id>
        <link href="https://optum.github.io/blog/2021/08/24/OSPO culture"/>
        <updated>2021-08-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This is my first post on Docusaurus 2.]]></summary>
        <content type="html"><![CDATA[<h3 class="anchor anchorWithStickyNavbar_mojV" id="what-is-an-ospo">What is an OSPO?<a class="hash-link" href="#what-is-an-ospo" title="Direct link to heading">​</a></h3><p>Today, we solidify our definition of what the United Healthgroup open source program office, or UHG OSPO, does. We are the center of competency for this organization's open source operations and structure. The Open Source Program Office (OSPO) owns and promotes the UHG open source strategy. After 7 years of service, we continue to focus on training and supporting developers in the open source domain. Our operations include setting code use, distribution, and selection. We train developers in methods of open contribution with support from Optum Technology University.</p><p>Companies create OSPOs to manage their relationships with the open source ecosystems they depend upon. It collaborates with engineering, legal, security, invention, product, technology, scientific, and strategic Open Source Software (OSS) communities to verify or adapt our strategy to implement OSS Governance as an enabler for employees to shape and influence technology needed to solve tomorrow’s problems. This provides the unique opportunity to stoke innovation, realize savings and encourage an engineering mindset, while supporting the enterprise’s social responsibility initiatives.</p><p>The Open Source Program Office (OSPO) owns and promotes the UHG open source strategy. It collaborates with engineering, legal, security, invention, product, technology, scientific, and strategic Open Source Software (OSS) communities to verify or adapt our strategy to implement OSS Governance as an enabler for employees to shape and influence technology needed to solve tomorrow’s problems. This provides the unique opportunity to stoke innovation, realize savings and encourage an engineering mindset, while supporting the enterprise’s social responsibility initiatives.</p><p>At their crux, OSPOs manage the inbound and outbound flow of open source projects in their company's ecosystem. The company's software health and sustainability are partly dependent on the Open Source Software ecosystem surrounding these activities. In one role, OSPOs are rangers of the organization's OSS ecosystem. In another, OSPOs are promoters of action in an open source community. OSPOs are inventors sitting at the intersection between technology and social responsibility.</p><p>At UHG, our OSPO exists to accomplish a set of primary objectives.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="primary-objectives">Primary Objectives<a class="hash-link" href="#primary-objectives" title="Direct link to heading">​</a></h3><ul><li>Continually Evolve OSS Policy and Process to Support Engineering and Business Communities</li><li>Drive Increased OSS Policy and Process Knowledge Across the Organization</li><li>Support Increased OSS Policy Compliance to Reduce Risk</li><li>Reduce Engineering Friction across the Organization</li><li>Increase Speed to Minimal Viable Products (MVPs) through Open Source Code Use and ReUse</li><li>Reduce external open source dependencies</li><li>Continually Educate Stakeholders on OSPO Capabilities and Value</li><li>Contribute to and Balance the Portfolio of Intellectual Property and Open Source Contribution</li><li>Elevate the Technical Eminence of the Healthcare Technology Industry</li></ul><p>As open-source leaders, we positively disrupt the healthcare and technology industries. We are a vibrant, celebrated, and supported open-source community. We proudly deliver innovative solutions to the world's toughest healthcare problems. Join us!</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="resources-and-references">Resources and References<a class="hash-link" href="#resources-and-references" title="Direct link to heading">​</a></h3><p>Optum Open Source. <a href="https://optum.github.io/" target="_blank" rel="noopener noreferrer">https://optum.github.io/</a></p><p>TODO. <a href="https://todogroup.org/blog/ospo-definition/" target="_blank" rel="noopener noreferrer">https://todogroup.org/blog/ospo-definition/</a></p><p>CHAOSS. <a href="https://chaoss.community/" target="_blank" rel="noopener noreferrer">https://chaoss.community/</a></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="get-involved">Get Involved<a class="hash-link" href="#get-involved" title="Direct link to heading">​</a></h3><p>Email <a href="mailto:opensource@optum.com" target="_blank" rel="noopener noreferrer">opensource@optum.com</a></p><hr><h4 class="anchor anchorWithStickyNavbar_mojV" id="disclaimer">Disclaimer<a class="hash-link" href="#disclaimer" title="Direct link to heading">​</a></h4><p>The information presented on this site about systems, technologies, and services are the express views of the authors and do not constitute endorsement, recommendation, or favoring by United Health group and its subsidiaries.</p>]]></content>
        <author>
            <name>DB Babjack</name>
            <uri>https://github.com/dbabjaxy</uri>
        </author>
        <category label="culture" term="culture"/>
        <category label="OSPO" term="OSPO"/>
    </entry>
</feed>