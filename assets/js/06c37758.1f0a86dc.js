"use strict";(self.webpackChunkoptum_github_io=self.webpackChunkoptum_github_io||[]).push([[8041],{3905:function(e,t,a){a.d(t,{Zo:function(){return p},kt:function(){return d}});var n=a(7294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=c(a),d=i,h=m["".concat(l,".").concat(d)]||m[d]||u[d]||r;return a?n.createElement(h,o(o({ref:t},p),{},{components:a})):n.createElement(h,o({ref:t},p))}));function d(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,o=new Array(r);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,o[1]=s;for(var c=2;c<r;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},2500:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return c},assets:function(){return p},toc:function(){return u},default:function(){return d}});var n=a(7462),i=a(3366),r=(a(7294),a(3905)),o=["components"],s={title:"Migrating ETL to Spark - Refactoring",author:"Bill Schneider",author_title:"Sr. Principal Engineer",author_url:"https://www.linkedin.com/in/wrschneider",author_image_url:"https://avatars.githubusercontent.com/u/3975157?v=4",tags:["Spark","ETL","Engineering"],hide_table_of_contents:!1},l=void 0,c={permalink:"/blog/2022/02/08/spark-part2-refactoring",source:"@site/blog/2022-02-08-spark-part2-refactoring.md",title:"Migrating ETL to Spark - Refactoring",description:"_Note: this is the second article in a multi-part series. The first post covered",date:"2022-02-08T00:00:00.000Z",formattedDate:"February 8, 2022",tags:[{label:"Spark",permalink:"/blog/tags/spark"},{label:"ETL",permalink:"/blog/tags/etl"},{label:"Engineering",permalink:"/blog/tags/engineering"}],readingTime:6.4,truncated:!0,authors:[{name:"Bill Schneider",title:"Sr. Principal Engineer",url:"https://www.linkedin.com/in/wrschneider",imageURL:"https://avatars.githubusercontent.com/u/3975157?v=4"}],frontMatter:{title:"Migrating ETL to Spark - Refactoring",author:"Bill Schneider",author_title:"Sr. Principal Engineer",author_url:"https://www.linkedin.com/in/wrschneider",author_image_url:"https://avatars.githubusercontent.com/u/3975157?v=4",tags:["Spark","ETL","Engineering"],hide_table_of_contents:!1},nextItem:{title:"Data Streaming At Scale With Benthos",permalink:"/blog/2022/01/25/data-streaming-at-scale-with-benthos"}},p={authorsImageUrls:[void 0]},u=[{value:"When SQL isn&#39;t enough",id:"when-sql-isnt-enough",children:[],level:3},{value:"Stay DRY with the DataFrame API",id:"stay-dry-with-the-dataframe-api",children:[],level:3},{value:"Get complex business logic out of SQL",id:"get-complex-business-logic-out-of-sql",children:[],level:3},{value:"Coming up",id:"coming-up",children:[],level:3}],m={toc:u};function d(e){var t=e.components,a=(0,i.Z)(e,o);return(0,r.kt)("wrapper",(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Note: this is the second article in a multi-part series. The ",(0,r.kt)("a",{parentName:"em",href:"https://opensource.optum.com/blog/2022/01/14/spark-series-part-1"},"first post")," covered\ngetting started by copy-pasting SQL into Spark with some rewrites. Future installments will cover topics like performance optimization and validation.")),(0,r.kt)("h3",{id:"when-sql-isnt-enough"},"When SQL isn't enough"),(0,r.kt)("p",null,'First, the term "Spark SQL" can be confusing. ',(0,r.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/sql-programming-guide.html"},'"Spark SQL"'),' refers to the structured data processing module within Spark. You can interact with Spark SQL through SQL SELECT queries, and through the DataFrame/Dataset API. For the rest of this article, when I say "SQL" I am referring to SQL syntax.'),(0,r.kt)("p",null,"At Optum, we often start migrating database-backed processes to Spark by copy-pasting SQL at first, then refactor to use the DataFrame or Dataset API when it makes sense."),(0,r.kt)("p",null,"The two main scenarios where it is useful to refactor SQL-based code:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},'DRY for transforming query results. You can use the power of Scala and functional programming to apply "for each column" logic.'),(0,r.kt)("li",{parentName:"ul"},"Complex business rules that are hard to express in SQL. Spark can distribute computations that go beyond the SQL set operations, and a row in a dataframe can contain nested objects, not just values.")),(0,r.kt)("h3",{id:"stay-dry-with-the-dataframe-api"},"Stay DRY with the DataFrame API"),(0,r.kt)("p",null,"One simple example is, suppose your original SQL query from Oracle or SQL Server selected all the column names as upper-case and you want to ensure that the generated Parquet column names are all lower-case. Rather than trying to parse the SQL text and replace all the column names, you can apply this sort of logic on the DataFrame itself:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'val result: DataFrame = spark.sql("SELECT ....")\n\nresult.schema.fieldNames.foldLeft(result) { (colName, df) => df.withColumnRenamed(colName, colName.toLowerCase()) }\n')),(0,r.kt)("p",null,"There is an explanation of how ",(0,r.kt)("inlineCode",{parentName:"p"},"foldLeft")," works on DataFrames ",(0,r.kt)("a",{parentName:"p",href:"https://stackoverflow.com/a/52028279/836318"},"on this StackOverflow post"),". The above code is looping over the column names on the original DataFrame, and for each column, returns a new DataFrame that renames the original column to the lower-case name. The functional syntax abstracts away reassignment of the DataFrame for each column, keeping all variables immutable."),(0,r.kt)("p",null,"You can use the DataFrame API to replace SQL syntax altogether, which is especially useful when you have repeating patterns."),(0,r.kt)("p",null,"For example if you have a query like this"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT cost1 * factor_val, cost2 * factor_val, cost3 * factor_val, ...\nFROM costs\nJOIN lookup_factor on costs.factor_id = lookup_factor.factor_id\n")),(0,r.kt)("p",null,"Where you have a series of identical, numbered columns all multiplied by some factor from a lookup table."),(0,r.kt)("p",null,"This can be rewritten as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'val costColumnRange = (1 to 10)\n\nval joined = spark.table("costs")\n  .join(spark.table("lookup_factor"), Seq("factor_id"))\n\nval costColumnsMultiplied = costColumnRange.map(i => col(s"cost$i") * $"factor")\nval final = joined.select(costColumnsMultiplied)\n')),(0,r.kt)("p",null,"Here, we start with a DataFrame that represents the two tables, ",(0,r.kt)("inlineCode",{parentName:"p"},"costs")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"lookup_factor"),", joined. No columns are specified at this point, so the ",(0,r.kt)("inlineCode",{parentName:"p"},"joined")," DataFrame has all columns from the original tables available. This is equivalent to ",(0,r.kt)("inlineCode",{parentName:"p"},"select * from costs join lookup_factor"),"."),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"map")," transforms the range of column suffixes in ",(0,r.kt)("inlineCode",{parentName:"p"},"costColumnRange")," to a list of Spark ",(0,r.kt)("inlineCode",{parentName:"p"},"Column"),' objects, where each Column represents a single "costX ',"*",' factor" column that would appear in a ',(0,r.kt)("inlineCode",{parentName:"p"},"SELECT")," statmeent."),(0,r.kt)("p",null,"The final result is produced by passing the list of ",(0,r.kt)("inlineCode",{parentName:"p"},"Column")," objects to ",(0,r.kt)("inlineCode",{parentName:"p"},"select"),"."),(0,r.kt)("p",null,"This construct is useful for patterns that repeat with similar column names or column names with numeric indexes."),(0,r.kt)("h3",{id:"get-complex-business-logic-out-of-sql"},"Get complex business logic out of SQL"),(0,r.kt)("p",null,"Sometimes complex analytics are calculated in SQL. For example, clinical quality measures typically follow a form like:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Patient qualifies for inclusion in metric denominator, by looking at procedure codes for an eligible visit in some time range"),(0,r.kt)("li",{parentName:"ul"},"Patient is not excluded from the calculation for some reason; for example, if a patient can't get a flu shot because of an allergy, that patient is not included counted in the deonominator"),(0,r.kt)("li",{parentName:"ul"},"Patient satisfies the metric numerator; for example, the patient got their flu shot.")),(0,r.kt)("p",null,"In SQL, a typical implementation is building temp tables for each measure. In this approach, each SQL query represents one component calculated for all patients:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"create table eligible_patients as\nselect distinct patient_id\nfrom patient\njoin patient_procedure using (patient_id)\nwhere procedure_code in (codes for eligible visit)\nand not exists (select 1\n   from patient_diagnosis\n   where patient_diagnosis.patient_id = patient_procedure.patient_id\n   and diagnosis_code in (codes that indicate exclusion)\n)\n\ncreate table patient_numerators as\nselect distinct patient_id\nfrom patients\njoin procedures using (patient_id)\nwhere procedure_code in (codes that indicate numerator satisfied)\n\ncreate table measure as\nselect count(numerator.patient_id)/count(denominator.patient_id) as measure\nfrom eligible_patients denominator\nleft join patient_numerators as numerator\n")),(0,r.kt)("p",null,"This is workable for one measure, but what happens when you need to calculate dozens of such measures? This kind of logic usually ends up requiring separate scans on tables like ",(0,r.kt)("inlineCode",{parentName:"p"},"patient_diagnosis")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"patient_procedure")," for each measure, whether a separate temp table or a separate ",(0,r.kt)("inlineCode",{parentName:"p"},"[not] exists")," subquery. So each scan ends up calcuating one measure or measure precursor for all patients at once."),(0,r.kt)("p",null,"In Spark, you are not limited to SQL set operations. You can treat rows in DataFrames like objects with nested collections (lists or sets), and use any procedural or functional logic on these objects. Then, you can take a completely different approach:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"fetch the all the related records for a patient at once, as a single Patient object"),(0,r.kt)("li",{parentName:"ul"},"for each patient, calculate all the measures for the patient in memory.")),(0,r.kt)("p",null,"The first step in this approach is to retrieve the data into an object graph so it's easier to work with:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'case class Diagnosis(diagnosis_code: String, ...)\ncase class Procedure(procedure_code: String, ...)\ncase class Patient(patient_id: Long, dx: Seq[Diagnosis], px: Seq[Procedure])\n\nval groupedDx = spark.table("patient_diagnosis")\n  .groupBy("patient_id")\n  .agg(collect_list(struct("diagnosis_code", ... ).as("diagnosis_list")))\n\nval groupedPx = spark.table("patient_procedure")\n  .groupBy("patient_id")\n  .agg(collect_list(struct("procedure_code", ... ).as("procedure_list")))\n\nval patients = spark.table("patient")\n  .join(groupedDx, Seq("patient_id"))\n  .join(groupedPx, Seq("patient_id"))\n  .as[Patient]\n')),(0,r.kt)("p",null,"This code aggregates the records from ",(0,r.kt)("inlineCode",{parentName:"p"},"patient_procedure")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"patient_diagnosis")," tables into temporary DataFrames with a single row for each patient. Each row has two fields: ",(0,r.kt)("inlineCode",{parentName:"p"},"patient_id"),", and an embedded list of procedure or diagnosis codes."),(0,r.kt)("p",null,"Since each of these temporary DataFrames only contains one record per patient, they can be joined safely. We could never do a join like this in SQL because a join between ",(0,r.kt)("inlineCode",{parentName:"p"},"patient_procedure")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"patient_diagnosis")," would be similar to a cartesian join (more precisely, a cartesian join within single patient, for the procedure and diagnosis codes that match our criteria)."),(0,r.kt)("p",null,"The final call ",(0,r.kt)("inlineCode",{parentName:"p"},".as[Patient]")," turns the Spark DataFrame into a typed Dataset, where each row is a ",(0,r.kt)("inlineCode",{parentName:"p"},"Patient")," ",(0,r.kt)("em",{parentName:"p"},"object"),", defined by the case classes at the beginning. Fields and collections in these objects may then be accessed or iterated, as if they were plain Scala objects."),(0,r.kt)("p",null,"This code will take that set of Patients, and call ",(0,r.kt)("inlineCode",{parentName:"p"},"calculateQualityMeasures")," on each one individually (in parallel, with different partitions of patients on different Spark executors), resulting in a Dataset of ",(0,r.kt)("inlineCode",{parentName:"p"},"QualityMeasure")," objects:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"case class QualityMeasure(eligible1: Boolean, satisfies1: Boolean, eligible2: Boolean, satisfies2: Boolean, ...)\n\ndef calculateQualityMeasures(p: Patient): QualityMeasure = {\n  val eligible1 = !p.px.intersect(ELIGIBLE_VISIT_PX_CODES_1).empty && p.dx.intersect(EXCLUDE_VISIT_DX_CODES_1).empty\n  val satisfies1 = !p.px.intersect(PX_CODES_1).empty\n\n  val eligible2 = !p.px.intersect(ELIGIBLE_VISIT_PX_CODES_2).empty && p.dx.intersect(EXCLUDE_VISIT_DX_CODES_2).empty\n  val satisfies2 = !p.px.intersect(PX_CODES_2).empty\n\n  QualityMeasure(eligible1, satisfies1, eligible2, satisfies2)\n}\n\nval results = patients.map(calculateQualityMeasures)\n")),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"map")," method on the Dataset acts like ",(0,r.kt)("inlineCode",{parentName:"p"},"map")," on any other Scala collection, where ",(0,r.kt)("inlineCode",{parentName:"p"},"calculateQualityMeasures")," is a pure function that takes a Patient in and returns QualityMeasures out."),(0,r.kt)("p",null,"Note that ",(0,r.kt)("inlineCode",{parentName:"p"},"calculateQualityMeasures")," has no Spark dependencies and can be unit-tested on Scala case class instances independently!"),(0,r.kt)("p",null,"Because the Patient object has the full collection of procedure and diagnosis codes in memory, we can perform multiple iterations over the same procedure and diagnosis codes for the patient to calculate all quality measures at the same time. This is a big difference from SQL, where logic operations like ",(0,r.kt)("inlineCode",{parentName:"p"},"where exists")," are coupled to storage operations to fetch data from disk."),(0,r.kt)("h3",{id:"coming-up"},"Coming up"),(0,r.kt)("p",null,"The next article will discuss testing and validation strategies."))}d.isMDXComponent=!0}}]);