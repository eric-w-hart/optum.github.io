"use strict";(self.webpackChunkoptum_github_io=self.webpackChunkoptum_github_io||[]).push([[4438],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return h}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=c(n),h=r,m=p["".concat(l,".").concat(h)]||p[h]||d[h]||o;return n?a.createElement(m,i(i({ref:t},u),{},{components:n})):a.createElement(m,i({ref:t},u))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=n[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},202:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return c},assets:function(){return u},toc:function(){return d},default:function(){return h}});var a=n(7462),r=n(3366),o=(n(7294),n(3905)),i=["components"],s={title:"Packaging models and analytics for reuse -  API vs. Inner Source.",author:"Bill Schneider",author_title:"Sr. Principal Engineer",author_url:"https://www.linkedin.com/in/wrschneider",author_image_url:"https://github.com/wrschneider.png",tags:["ML-AI"],hide_table_of_contents:!1},l=void 0,c={permalink:"/blog/2021/11/15/package-model-analytic-reuse",source:"@site/blog/2021-11-15-package-model-analytic-reuse.md",title:"Packaging models and analytics for reuse -  API vs. Inner Source.",description:"Packaging models and analytics for reuse - API vs. Inner Source.",date:"2021-11-15T00:00:00.000Z",formattedDate:"November 15, 2021",tags:[{label:"ML-AI",permalink:"/blog/tags/ml-ai"}],readingTime:3.68,truncated:!0,authors:[{name:"Bill Schneider",title:"Sr. Principal Engineer",url:"https://www.linkedin.com/in/wrschneider",imageURL:"https://github.com/wrschneider.png"}],frontMatter:{title:"Packaging models and analytics for reuse -  API vs. Inner Source.",author:"Bill Schneider",author_title:"Sr. Principal Engineer",author_url:"https://www.linkedin.com/in/wrschneider",author_image_url:"https://github.com/wrschneider.png",tags:["ML-AI"],hide_table_of_contents:!1},prevItem:{title:"Migrating ETL to Spark - Motivation and Quick Start",permalink:"/blog/2022/01/14/spark-series-part-1"},nextItem:{title:"Copying data between AWS and Azure",permalink:"/blog/2021/10/21/s3-to-azure-copy"}},u={authorsImageUrls:[void 0]},d=[{value:"Packaging models and analytics for reuse - API vs. Inner Source.",id:"packaging-models-and-analytics-for-reuse---api-vs-inner-source",children:[],level:3}],p={toc:d};function h(e){var t=e.components,n=(0,r.Z)(e,i);return(0,o.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h3",{id:"packaging-models-and-analytics-for-reuse---api-vs-inner-source"},"Packaging models and analytics for reuse - API vs. Inner Source."),(0,o.kt)("p",null,'Various teams in Optum produce models or analytics. Simplified examples of "models":'),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"risk scoring for a patient - given history for a patient, how likely are they to be admitted to the hospital as an inpatient?"),(0,o.kt)("li",{parentName:"ul"},"grouping claims into an episode - what is the all-in cost for a procedure including follow-up care?"),(0,o.kt)("li",{parentName:"ul"},"calculating quality measures - in a patient population, what percentage got their flu shots this year?")),(0,o.kt)("p",null,"Models can be either rules-based and explicitly coded, or ML-based."),(0,o.kt)("p",null,"Once we have models like these, how can that IP be reused in a different context? The goal should be to be able to\ncompose various individual models or analytics into new products."),(0,o.kt)("p",null,"There are a few different approaches typically used:"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"REST APIs")," The team that creates a model can deploy their model as a REST API for other teams to invoke. This\nanalytic-as-a-service approach\nworks well for APIs that are called inside of a workflow--for example, making a decision about an individual patient\nat point of care, or analytics that can be recomputed incrementally without full history."),(0,o.kt)("p",null,"The benefit of APIs is that the provider and consumer tech stack doesn't need to align, and that consumers will see changes to the model immediately when it is deployed with no action required. The big downside is latency: REST doesn't work well in batch workflows if processing each individual record requires an HTTPS round-trip. The other downside is operational coupling: the team that produces the model needs to not only maintain the logic of the model itself, but also is responsible for availability and performance for the REST API. Also the provider can't make breaking changes without an API versioning strategy, keeping the old versions of the API live until consumers can migrate."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Batch API")," A variation of the analytic-as-a-service approach to address the HTTPS round-trip issue with batch data, for analytics that involve aggregating historical data and can't recompute on incremental data alone. The payload goes through a file transfer (SFTP, Azure blob storage, AWS S3 etc.) and REST endpoints are only used to mediate the process with pointers\nto the out-of-band payload (similar to\n",(0,o.kt)("a",{parentName:"p",href:"https://docs.microsoft.com/en-us/azure/architecture/patterns/claim-check"},"claim check pattern"),"). Still, you have to ship\ndata back and forth and the file transfer time can add significant overhead to overall turnaround time. Also, you still\nhave operational coupling and concerns over breaking changes, although this could beinsulated from the end user interactions."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Inner source")," Instead of offering analytics as a service, you can package an analytic as an artifact that can be\nembedded as a dependency into any other application. This could either be a JAR or DLL that gets included as a build\ndependency, or the output of an ML framework (PyTorch, Tensorflow etc.)"),(0,o.kt)("p",null,"The benefits here are: you bring the analytic to the data, so there is no\noverhead in transferring data back and forth. The model provider has no operational responsibility or cost. Consumers\ncan continue using old versions of artifacts until they are ready to upgrade. The downsides are: consumers have to\ntake action to upgrade. There are tech stack dependencies -- if the analytic is a JAR and your team is standardized on\n.NET, or the ML framework doesn't have an SDK for your platform, you can't call it in-process. Tech stack alignment is\nless of an issue for batch process, since different stages of a batch workflow can be on different technologies as long as\nthey share a file system."),(0,o.kt)("p",null,"There is no one right answer, and we use different approaches in different situations. To make a decision you have to ask yourself a series of questions and decide how important each one is:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"How often does the underlying model/analytic change?"),(0,o.kt)("li",{parentName:"ul"},"As a consumer, do you want to see changes right away or do you want to be in control over moving to the new version?"),(0,o.kt)("li",{parentName:"ul"},"Is this a real-time interaction or a batch process?"),(0,o.kt)("li",{parentName:"ul"},"How big are concerns over data transfer overhead or request latency?"),(0,o.kt)("li",{parentName:"ul"},"If you need a real-time interaction, are you on compatible tech stacks?")),(0,o.kt)("p",null,"Also note that ",(0,o.kt)("em",{parentName:"p"},"these approaches are not mutually exclusive.")," If you are a team providing a model or analytic, and you\nthink about how to package your model for reuse via inner-source, you can ",(0,o.kt)("em",{parentName:"p"},"also")," provide that same model as a REST API endpoint."))}h.isMDXComponent=!0}}]);