"use strict";(self.webpackChunkoptum_github_io=self.webpackChunkoptum_github_io||[]).push([[2569],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return m}});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),p=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return r.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=p(n),m=a,h=d["".concat(l,".").concat(m)]||d[m]||u[m]||o;return n?r.createElement(h,i(i({ref:t},c),{},{components:n})):r.createElement(h,i({ref:t},c))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,i[1]=s;for(var p=2;p<o;p++)i[p]=n[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},674:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return p},assets:function(){return c},toc:function(){return u},default:function(){return m}});var r=n(7462),a=n(3366),o=(n(7294),n(3905)),i=["components"],s={title:"Copying data between AWS and Azure",author:"Bill Schneider",author_title:"Sr. Principal Engineer",author_url:"https://github.com/wrschneider",author_image_url:"https://github.com/wrschneider.png",tags:["AWS Azure S3 Cloud","engineering"],hide_table_of_contents:!1},l=void 0,p={permalink:"/blog/2021/10/21/s3-to-azure-copy",source:"@site/blog/2021-10-21-s3-to-azure-copy.md",title:"Copying data between AWS and Azure",description:"Copying data between AWS and Azure",date:"2021-10-21T00:00:00.000Z",formattedDate:"October 21, 2021",tags:[{label:"AWS Azure S3 Cloud",permalink:"/blog/tags/aws-azure-s-3-cloud"},{label:"engineering",permalink:"/blog/tags/engineering"}],readingTime:2.875,truncated:!0,authors:[{name:"Bill Schneider",title:"Sr. Principal Engineer",url:"https://github.com/wrschneider",imageURL:"https://github.com/wrschneider.png"}],frontMatter:{title:"Copying data between AWS and Azure",author:"Bill Schneider",author_title:"Sr. Principal Engineer",author_url:"https://github.com/wrschneider",author_image_url:"https://github.com/wrschneider.png",tags:["AWS Azure S3 Cloud","engineering"],hide_table_of_contents:!1},prevItem:{title:"Packaging models and analytics for reuse -  API vs. Inner Source.",permalink:"/blog/2021/11/15/package-model-analytic-reuse"},nextItem:{title:"Knative Channel Sans Kafka Admin Rights",permalink:"/blog/2021/09/27/kn-kafka-topic-channel"}},c={authorsImageUrls:[void 0]},u=[{value:"Copying data between AWS and Azure",id:"copying-data-between-aws-and-azure",children:[],level:3}],d={toc:u};function m(e){var t=e.components,n=(0,a.Z)(e,i);return(0,o.kt)("wrapper",(0,r.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h3",{id:"copying-data-between-aws-and-azure"},"Copying data between AWS and Azure"),(0,o.kt)("p",null,"Many Optum teams are working in Azure, although some are also working in AWS.  So sometimes we have to transfer files between\nAWS and Azure, for batch-processing pipelines that have components in both clouds."),(0,o.kt)("p",null,"The easiest way to move files between clouds is through their respective object storage APIs: Azure Blob Storage or\nAWS S3.  Transfers are over port 443 with public-facing endpoints, and authentication/authorization is handled like\nall other native cloud services."),(0,o.kt)("p",null,"There are good open-source tools for AWS/Azure file transfers, including\n",(0,o.kt)("a",{parentName:"p",href:"https://github.com/Azure/azure-storage-azcopy"},(0,o.kt)("inlineCode",{parentName:"a"},"azcopy"))," and\n",(0,o.kt)("a",{parentName:"p",href:"https://github.com/rclone/rclone"},(0,o.kt)("inlineCode",{parentName:"a"},"rclone")),".  Both are command-line tools that can be executed from automated\nprocesses.  ",(0,o.kt)("inlineCode",{parentName:"p"},"azcopy")," is specialized to work with Azure, but lacks the ability to move data from Azure to\nAWS S3.  ",(0,o.kt)("inlineCode",{parentName:"p"},"rclone")," is general purpose any-to-any transfers, but may not be as optimized for uploads to Azure."),(0,o.kt)("p",null,"These tools can easily be installed in Docker containers.  If you are running on AWS, you might want to execute\na transfer to/from Azure as a serverless ECS Fargate task.  When you run the task, you would pass the specific\ncommand line parameters in the ",(0,o.kt)("inlineCode",{parentName:"p"},"containerOverrides"),".  This is all standard; the only tricky piece is\nhow to manage credentials for both AWS and Azure.  Secrets that can be passed through environment variables can be\n",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html"},"stored in AWS Parameter Store or Secrets Manager and retrieved with ",(0,o.kt)("inlineCode",{parentName:"a"},"secrets")," in the container definition"),".  Others require wrapper scripts to perform\nadditional tasks before running azcopy or rclone."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"azcopy "),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"AWS credentials: for some reason, ",(0,o.kt)("a",{parentName:"li",href:"https://github.com/Azure/azure-storage-azcopy/issues/1341"},"azcopy doesn't use native AWS profiles")," and only takes AWS credentials from environment variables.  So you need a wrapper script to set these up."),(0,o.kt)("li",{parentName:"ul"},"Azure SAS tokens: azcopy expects these on the command line, so it's a good idea for a wrapper script to append this to the URL"),(0,o.kt)("li",{parentName:"ul"},"Azure service principal: secret can be passed through ",(0,o.kt)("inlineCode",{parentName:"li"},"AZCOPY_SPA_CLIENT_SECRET")," environment variable."))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Rclone "),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"AWS credentials: uses the AWS SDK and will pick up AWS credentials from the container's execution role."),(0,o.kt)("li",{parentName:"ul"},"Azure SAS tokens: SAS URL can be passed via ",(0,o.kt)("inlineCode",{parentName:"li"},"RCLONE_AZUREBLOB_SAS_URL")," environment variable as container secret."),(0,o.kt)("li",{parentName:"ul"},"Azure service principal: needs to be stored as JSON in a file.  Your container needs a wrapper script that takes\nthe actual secret from the injected environment variable, and writes it to a file.")))),(0,o.kt)("p",null,"Examples of wrapper script for ",(0,o.kt)("inlineCode",{parentName:"p"},"azcopy")," to set up AWS credentials from the ECS metadata endpoint:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"response=$(wget -O - 169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI)\nexport AWS_ACCESS_KEY_ID=$(echo \"$response\" | jq -r '.AccessKeyId')\nexport AWS_SECRET_ACCESS_KEY=$(echo \"$response\" | jq -r '.SecretAccessKey')\nexport AWS_SESSION_TOKEN=$(echo \"$response\" | jq -r '.Token')\nazcopy $@\n")),(0,o.kt)("p",null,"If you are copying to Azure and need to keep the SAS token secret, you can append modify the above script to\nappend it to the URL and then omit the SAS token from the Azure blob storage URL passed in ",(0,o.kt)("inlineCode",{parentName:"p"},"containerOverrides"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"azcopy $@$AZURE_SECRET\n")),(0,o.kt)("p",null,"Note that this only works because azcopy can only be used to copy ",(0,o.kt)("em",{parentName:"p"},"from")," S3 ",(0,o.kt)("em",{parentName:"p"},"to")," Azure, not the other way around, so\nwe know the Azure URL will always be the final argument on the command line."),(0,o.kt)("p",null,"For rclone, this is a wrapper script to handle the service principal JSON:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},'# assume AZURE_SECRET is configured with secret in container definition\necho "$AZURE_SECRET" > /tmp/secret.json\nexport RCLONE_AZUREBLOB_SERVICE_PRINCIPAL_FILE=/tmp/secret.json\nrclone $@\n')),(0,o.kt)("p",null,"All of the above will allow you to do your Azure/AWS file transfers in a serverless Fargate task, that can be\nintegrated with whatever other pipeline workflow you have."))}m.isMDXComponent=!0}}]);