"use strict";(self.webpackChunkoptum_github_io=self.webpackChunkoptum_github_io||[]).push([[1477],{10:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"/2022/02/08/spark-part2-refactoring","metadata":{"permalink":"/blog/2022/02/08/spark-part2-refactoring","source":"@site/blog/2022-02-08-spark-part2-refactoring.md","title":"Migrating ETL to Spark - Refactoring","description":"_Note: this is the second article in a multi-part series. The first post covered","date":"2022-02-08T00:00:00.000Z","formattedDate":"February 8, 2022","tags":[{"label":"Spark","permalink":"/blog/tags/spark"},{"label":"ETL","permalink":"/blog/tags/etl"},{"label":"Engineering","permalink":"/blog/tags/engineering"}],"readingTime":6.4,"truncated":true,"authors":[{"name":"Bill Schneider","title":"Sr. Principal Engineer","url":"https://www.linkedin.com/in/wrschneider","imageURL":"https://avatars.githubusercontent.com/u/3975157?v=4"}],"frontMatter":{"title":"Migrating ETL to Spark - Refactoring","author":"Bill Schneider","author_title":"Sr. Principal Engineer","author_url":"https://www.linkedin.com/in/wrschneider","author_image_url":"https://avatars.githubusercontent.com/u/3975157?v=4","tags":["Spark","ETL","Engineering"],"hide_table_of_contents":false},"nextItem":{"title":"Data Streaming At Scale With Benthos","permalink":"/blog/2022/01/25/data-streaming-at-scale-with-benthos"}},"content":"_Note: this is the second article in a multi-part series. The [first post](https://opensource.optum.com/blog/2022/01/14/spark-series-part-1) covered\\ngetting started by copy-pasting SQL into Spark with some rewrites. Future installments will cover topics like performance optimization and validation._\\n\\n### When SQL isn\'t enough\\n\\n\x3c!--truncate--\x3e\\n\\nFirst, the term \\"Spark SQL\\" can be confusing. [\\"Spark SQL\\"](https://spark.apache.org/docs/latest/sql-programming-guide.html) refers to the structured data processing module within Spark. You can interact with Spark SQL through SQL SELECT queries, and through the DataFrame/Dataset API. For the rest of this article, when I say \\"SQL\\" I am referring to SQL syntax.\\n\\nAt Optum, we often start migrating database-backed processes to Spark by copy-pasting SQL at first, then refactor to use the DataFrame or Dataset API when it makes sense.\\n\\nThe two main scenarios where it is useful to refactor SQL-based code:\\n\\n- DRY for transforming query results. You can use the power of Scala and functional programming to apply \\"for each column\\" logic.\\n- Complex business rules that are hard to express in SQL. Spark can distribute computations that go beyond the SQL set operations, and a row in a dataframe can contain nested objects, not just values.\\n\\n### Stay DRY with the DataFrame API\\n\\nOne simple example is, suppose your original SQL query from Oracle or SQL Server selected all the column names as upper-case and you want to ensure that the generated Parquet column names are all lower-case. Rather than trying to parse the SQL text and replace all the column names, you can apply this sort of logic on the DataFrame itself:\\n\\n```scala\\nval result: DataFrame = spark.sql(\\"SELECT ....\\")\\n\\nresult.schema.fieldNames.foldLeft(result) { (colName, df) => df.withColumnRenamed(colName, colName.toLowerCase()) }\\n```\\n\\nThere is an explanation of how `foldLeft` works on DataFrames [on this StackOverflow post](https://stackoverflow.com/a/52028279/836318). The above code is looping over the column names on the original DataFrame, and for each column, returns a new DataFrame that renames the original column to the lower-case name. The functional syntax abstracts away reassignment of the DataFrame for each column, keeping all variables immutable.\\n\\nYou can use the DataFrame API to replace SQL syntax altogether, which is especially useful when you have repeating patterns.\\n\\nFor example if you have a query like this\\n\\n```sql\\nSELECT cost1 * factor_val, cost2 * factor_val, cost3 * factor_val, ...\\nFROM costs\\nJOIN lookup_factor on costs.factor_id = lookup_factor.factor_id\\n```\\n\\nWhere you have a series of identical, numbered columns all multiplied by some factor from a lookup table.\\n\\nThis can be rewritten as follows:\\n\\n```scala\\nval costColumnRange = (1 to 10)\\n\\nval joined = spark.table(\\"costs\\")\\n  .join(spark.table(\\"lookup_factor\\"), Seq(\\"factor_id\\"))\\n\\nval costColumnsMultiplied = costColumnRange.map(i => col(s\\"cost$i\\") * $\\"factor\\")\\nval final = joined.select(costColumnsMultiplied)\\n```\\n\\nHere, we start with a DataFrame that represents the two tables, `costs` and `lookup_factor`, joined. No columns are specified at this point, so the `joined` DataFrame has all columns from the original tables available. This is equivalent to `select * from costs join lookup_factor`.\\n\\nThe `map` transforms the range of column suffixes in `costColumnRange` to a list of Spark `Column` objects, where each Column represents a single \\"costX \\\\* factor\\" column that would appear in a `SELECT` statmeent.\\n\\nThe final result is produced by passing the list of `Column` objects to `select`.\\n\\nThis construct is useful for patterns that repeat with similar column names or column names with numeric indexes.\\n\\n### Get complex business logic out of SQL\\n\\nSometimes complex analytics are calculated in SQL. For example, clinical quality measures typically follow a form like:\\n\\n- Patient qualifies for inclusion in metric denominator, by looking at procedure codes for an eligible visit in some time range\\n- Patient is not excluded from the calculation for some reason; for example, if a patient can\'t get a flu shot because of an allergy, that patient is not included counted in the deonominator\\n- Patient satisfies the metric numerator; for example, the patient got their flu shot.\\n\\nIn SQL, a typical implementation is building temp tables for each measure. In this approach, each SQL query represents one component calculated for all patients:\\n\\n```sql\\ncreate table eligible_patients as\\nselect distinct patient_id\\nfrom patient\\njoin patient_procedure using (patient_id)\\nwhere procedure_code in (codes for eligible visit)\\nand not exists (select 1\\n   from patient_diagnosis\\n   where patient_diagnosis.patient_id = patient_procedure.patient_id\\n   and diagnosis_code in (codes that indicate exclusion)\\n)\\n\\ncreate table patient_numerators as\\nselect distinct patient_id\\nfrom patients\\njoin procedures using (patient_id)\\nwhere procedure_code in (codes that indicate numerator satisfied)\\n\\ncreate table measure as\\nselect count(numerator.patient_id)/count(denominator.patient_id) as measure\\nfrom eligible_patients denominator\\nleft join patient_numerators as numerator\\n```\\n\\nThis is workable for one measure, but what happens when you need to calculate dozens of such measures? This kind of logic usually ends up requiring separate scans on tables like `patient_diagnosis` or `patient_procedure` for each measure, whether a separate temp table or a separate `[not] exists` subquery. So each scan ends up calcuating one measure or measure precursor for all patients at once.\\n\\nIn Spark, you are not limited to SQL set operations. You can treat rows in DataFrames like objects with nested collections (lists or sets), and use any procedural or functional logic on these objects. Then, you can take a completely different approach:\\n\\n- fetch the all the related records for a patient at once, as a single Patient object\\n- for each patient, calculate all the measures for the patient in memory.\\n\\nThe first step in this approach is to retrieve the data into an object graph so it\'s easier to work with:\\n\\n```scala\\ncase class Diagnosis(diagnosis_code: String, ...)\\ncase class Procedure(procedure_code: String, ...)\\ncase class Patient(patient_id: Long, dx: Seq[Diagnosis], px: Seq[Procedure])\\n\\nval groupedDx = spark.table(\\"patient_diagnosis\\")\\n  .groupBy(\\"patient_id\\")\\n  .agg(collect_list(struct(\\"diagnosis_code\\", ... ).as(\\"diagnosis_list\\")))\\n\\nval groupedPx = spark.table(\\"patient_procedure\\")\\n  .groupBy(\\"patient_id\\")\\n  .agg(collect_list(struct(\\"procedure_code\\", ... ).as(\\"procedure_list\\")))\\n\\nval patients = spark.table(\\"patient\\")\\n  .join(groupedDx, Seq(\\"patient_id\\"))\\n  .join(groupedPx, Seq(\\"patient_id\\"))\\n  .as[Patient]\\n```\\n\\nThis code aggregates the records from `patient_procedure` and `patient_diagnosis` tables into temporary DataFrames with a single row for each patient. Each row has two fields: `patient_id`, and an embedded list of procedure or diagnosis codes.\\n\\nSince each of these temporary DataFrames only contains one record per patient, they can be joined safely. We could never do a join like this in SQL because a join between `patient_procedure` and `patient_diagnosis` would be similar to a cartesian join (more precisely, a cartesian join within single patient, for the procedure and diagnosis codes that match our criteria).\\n\\nThe final call `.as[Patient]` turns the Spark DataFrame into a typed Dataset, where each row is a `Patient` _object_, defined by the case classes at the beginning. Fields and collections in these objects may then be accessed or iterated, as if they were plain Scala objects.\\n\\nThis code will take that set of Patients, and call `calculateQualityMeasures` on each one individually (in parallel, with different partitions of patients on different Spark executors), resulting in a Dataset of `QualityMeasure` objects:\\n\\n```\\ncase class QualityMeasure(eligible1: Boolean, satisfies1: Boolean, eligible2: Boolean, satisfies2: Boolean, ...)\\n\\ndef calculateQualityMeasures(p: Patient): QualityMeasure = {\\n  val eligible1 = !p.px.intersect(ELIGIBLE_VISIT_PX_CODES_1).empty && p.dx.intersect(EXCLUDE_VISIT_DX_CODES_1).empty\\n  val satisfies1 = !p.px.intersect(PX_CODES_1).empty\\n\\n  val eligible2 = !p.px.intersect(ELIGIBLE_VISIT_PX_CODES_2).empty && p.dx.intersect(EXCLUDE_VISIT_DX_CODES_2).empty\\n  val satisfies2 = !p.px.intersect(PX_CODES_2).empty\\n\\n  QualityMeasure(eligible1, satisfies1, eligible2, satisfies2)\\n}\\n\\nval results = patients.map(calculateQualityMeasures)\\n```\\n\\nThe `map` method on the Dataset acts like `map` on any other Scala collection, where `calculateQualityMeasures` is a pure function that takes a Patient in and returns QualityMeasures out.\\n\\nNote that `calculateQualityMeasures` has no Spark dependencies and can be unit-tested on Scala case class instances independently!\\n\\nBecause the Patient object has the full collection of procedure and diagnosis codes in memory, we can perform multiple iterations over the same procedure and diagnosis codes for the patient to calculate all quality measures at the same time. This is a big difference from SQL, where logic operations like `where exists` are coupled to storage operations to fetch data from disk.\\n\\n### Coming up\\n\\nThe next article will discuss testing and validation strategies."},{"id":"/2022/01/25/data-streaming-at-scale-with-benthos","metadata":{"permalink":"/blog/2022/01/25/data-streaming-at-scale-with-benthos","source":"@site/blog/2022-01-25-data-streaming-at-scale-with-benthos.md","title":"Data Streaming At Scale With Benthos","description":"Data Streaming At Scale With Benthos","date":"2022-01-25T00:00:00.000Z","formattedDate":"January 25, 2022","tags":[{"label":"Benthos","permalink":"/blog/tags/benthos"},{"label":"Golang","permalink":"/blog/tags/golang"},{"label":"Kafka","permalink":"/blog/tags/kafka"},{"label":"Data Streaming","permalink":"/blog/tags/data-streaming"},{"label":"ETL","permalink":"/blog/tags/etl"},{"label":"Engineering","permalink":"/blog/tags/engineering"}],"readingTime":2.7,"truncated":true,"authors":[{"name":"Mihai Todor","title":"Principal Software Engineer","url":"https://www.linkedin.com/in/mtodor","imageURL":"https://github.com/mihaitodor.png"}],"frontMatter":{"title":"Data Streaming At Scale With Benthos","author":"Mihai Todor","author_title":"Principal Software Engineer","author_url":"https://www.linkedin.com/in/mtodor","author_image_url":"https://github.com/mihaitodor.png","tags":["Benthos","Golang","Kafka","Data Streaming","ETL","Engineering"],"hide_table_of_contents":false},"prevItem":{"title":"Migrating ETL to Spark - Refactoring","permalink":"/blog/2022/02/08/spark-part2-refactoring"},"nextItem":{"title":"Migrating ETL to Spark - Motivation and Quick Start","permalink":"/blog/2022/01/14/spark-series-part-1"}},"content":"### Data Streaming At Scale With Benthos\\n\\n\x3c!--truncate--\x3e\\n\\nOptum\'s [Open Source Program Office (OSPO)](https://opensource.optum.com/blog/2021/08/24/OSPO%20culture) recently launched an Open Source sponsorship program, committing an initial $50K total donations to external Open Source projects of notable merit. [Benthos](https://benthos.dev/), the stream processor for mundane tasks, was selected as the first project to receive a donation through this program.\\n\\nThis article is the first part of a longer series of posts about Benthos, where I\'ll share some insight into why we decided to adopt Benthos as the core data streaming engine for Optum\'s new [Data Mesh](https://martinfowler.com/articles/data-mesh-principles.html) solution.\\n\\n![Benthos logo](./images/benthos.dev.svg)\\n\\nBenthos is a stateless data streaming engine written entirely in [Go](https://go.dev/) and compiled as a single static binary. It enables data engineers to configure a pipeline of processing steps that are to be executed on messages as they flow from the source to the sink during runtime. Through back pressure mechanisms, it ensures at-least-once delivery when connecting to sources and sinks which support this, without persisting in-flight messages in any temporary storage. It has a wide range of [builtin connectors](https://www.benthos.dev/docs/about/#components) that are expanding constantly and it\'s very easy to [deploy](https://www.benthos.dev/docs/guides/getting_started) in production. This project was started over 6 years ago by [Ashley Jeffs](https://www.jeffail.uk/) (he goes by Ash), who is still maintaining it and is currently [preparing](https://github.com/Jeffail/benthos/issues/1071) for the 4th major release.\\n\\nAt Optum, we have large volumes of disperse data in various formats spread across many different datastores, some of which are legacy systems that are slowly getting phased out. Benthos allows us to perform data movement from these systems to modern cloud-based datastores, where data analysts can perform their work more efficiently. While Benthos doesn\'t provide connectors for all the types of sources and sinks that we need, it does come with a rich [Golang API](https://pkg.go.dev/github.com/Jeffail/benthos/v3/public/service) for writing custom plugins that allow us to expand its functionality and tailor it for our needs.\\n\\nSince Benthos can easily be run in Kubernetes clusters, it enables us to build robust and efficient systems for terabyte-size workloads which scale horizontally. In order to monitor our production workloads, we leveraged the various Prometheus-compatible [metrics](https://www.benthos.dev/docs/components/metrics/about) and Jaeger-compatible [traces](https://www.benthos.dev/docs/components/tracers/about) that Benthos emits for each component of its configured processing pipeline. For complex workflows, it can be configured to add extra metrics using the [`metric` processor](https://www.benthos.dev/docs/components/processors/metric).\\n\\nAdditionally, we use distributed event streaming platforms, such as [Apache Kafka](https://kafka.apache.org/), as message buses for various realtime and batch ETL (Extract, Transform, Load) workflows. Benthos comes with an embedded DSL called [Bloblang](https://www.benthos.dev/docs/guides/bloblang/about), which lets us express complex transformations of structured data, such as schema migrations and validations.\\n\\nDuring our Benthos adoption, we pushed upstream several new adaptors and features that were quickly approved and merged. Getting code merged into Benthos has been one of my best experiences in contributing to Open Source so far.\\n\\n[Benthos Studio](https://studio.benthos.dev/) has been released recently to aid users in designing, visualising and testing Benthos streaming pipelines and it\'s proving very valuable when building new pipelines from scratch or navigating complex ones.\\n\\nIf you\'d like to learn more about Benthos, Ash\'s [YouTube channel](https://www.youtube.com/c/Jeffail) is a good place to start. Also, there is a vibrant [community](https://www.benthos.dev/community) around it, where Ash and other users like myself are available to answer any questions that you might have.\\n\\nTry [Benthos](https://benthos.dev/) today if you haven\'t already!"},{"id":"/2022/01/14/spark-series-part-1","metadata":{"permalink":"/blog/2022/01/14/spark-series-part-1","source":"@site/blog/2022-01-14-spark-series-part-1.md","title":"Migrating ETL to Spark - Motivation and Quick Start","description":"Note: this is the first article in a multi-part series. Future installments will cover topics like performance optimization, validation, and refactoring.","date":"2022-01-14T00:00:00.000Z","formattedDate":"January 14, 2022","tags":[{"label":"Spark","permalink":"/blog/tags/spark"},{"label":"ETL","permalink":"/blog/tags/etl"},{"label":"Engineering","permalink":"/blog/tags/engineering"}],"readingTime":5.38,"truncated":true,"authors":[{"name":"Bill Schneider","title":"Sr. Principal Engineer","url":"https://www.linkedin.com/in/wrschneider","imageURL":"https://avatars.githubusercontent.com/u/3975157?v=4"}],"frontMatter":{"title":"Migrating ETL to Spark - Motivation and Quick Start","author":"Bill Schneider","author_title":"Sr. Principal Engineer","author_url":"https://www.linkedin.com/in/wrschneider","author_image_url":"https://avatars.githubusercontent.com/u/3975157?v=4","tags":["Spark","ETL","Engineering"],"hide_table_of_contents":false},"prevItem":{"title":"Data Streaming At Scale With Benthos","permalink":"/blog/2022/01/25/data-streaming-at-scale-with-benthos"},"nextItem":{"title":"Packaging models and analytics for reuse -  API vs. Inner Source.","permalink":"/blog/2021/11/15/package-model-analytic-reuse"}},"content":"_Note: this is the first article in a multi-part series. Future installments will cover topics like performance optimization, validation, and refactoring._\\n\\n### Advantages of Spark\\n\\n\x3c!--truncate--\x3e\\n\\nAt Optum, we are migrating some of our ETL processes to [Apache Spark](https://spark.apache.org), an open-source framework for distributed computing. Many of these processes were previously running on relational databases, based on SQL queries and stored procedures. The data transformations can range from simple cleansing, to table joins and aggregations, to complex business logic. The results are then stored in a data warehouse for use in analytics applications.\\n\\nWhile DBs have their advatanges, there\'s a limit to how big of a dataset you can process in a given timeframe. That\'s because databases are generally meant to scale vertically -- when you have a bigger dataset, you get a bigger server. At some point, though, you can\'t make your server any bigger. Spark, like other big-data platforms, is meant to scale horizontally -- when you get a bigger dataset, you add _more_ servers.\\n\\nHorizontal scaling is especially useful when you run Spark in the public cloud, given the options for on-demand and serverless compute. Cloud gives the flexibility to scale up resources to meet demand rather than queueing jobs until there is capacity on a fixed server or Spark cluster. Then you pay for your compute resources by the CPU-hour, rather than having to size your cluster for the peak.\\n\\nSpark lets you build your data transformations with SQL syntax or SQL-like constructs, so there is a natural transition from a SQL-based ETL process to Spark. Rather than processing joins and aggregations on a single server, though, Spark datasets are distributed. Each node in a cluster works on a partition of the data in parallel. Intermediate results from one `JOIN` or `GROUP BY` are then redistributed, or _shuffled_, to different nodes for the next operation.\\n\\n### Getting started with Spark SQL\\n\\n_For this and future sections, I assume you have some familiarity with Spark and how to author and run Spark jobs. There are plenty of tutorials out there, including some good resources in the [Spark documentation](https://spark.apache.org/docs/latest/sql-getting-started.html)_\\n\\nSpark offers SQL syntax, but Spark is not a database! Spark datasets are in-memory, and are explicitly read or written from storage. The separation of storage and compute helps with horizontal scale as both can be scaled independently. Typically if our job is running in the cloud, data will be stored in object/blob storage.\\n\\nA typical ETL process might have something like this:\\n\\n```sql\\nCREATE TABLE patient_visits(patient_id bigint, patient_name varchar(100), visit_count int)\\nINSERT INTO patient_visits\\nSELECT patient_id, patient_name, count(1) visit_count\\nFROM  patient\\nJOIN  visit ON patient.patient_id = visit.patient_id\\nGROUP BY patient_id, patient_name\\n```\\n\\nIn Spark, there is no `INSERT` statement, only `SELECT`. When you run the SQL `SELECT` query in Spark SQL, you would get back a DataFrame, which is a tabular abstraction on top of a Spark Resilient Distributed Dataset (RDD). You would then write the DataFrame explicitly to storage:\\n\\n```scala\\nval df = spark.sql(\\"\\"\\"\\n  SELECT patient_id, patient_name, count(1) visit_count\\n  FROM  patient\\n  JOIN  visit ON patient.patient_id = visit.patient_id\\n  GROUP BY patient_id, patient_name\\n\\"\\"\\")\\nval targetLocation = // s3 or Azure blob store path\\ndf.write.parquet(targetLocation)\\n```\\n\\nThe output format is typically [Parquet](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html), which uses compressed and column-oriented storage. That means if you run a query that only reads two columns, you only retrieve the data for those two columns from storage. Most database tables use row-oriented storage by default which means you read all the columns from storage unless you can use a covering index. The Parquet format also contains schema information, like a database table. So unlike text formats, there is a distinction between `3` (integer), `3.000....` (floating point) and `3.00` (fixed-point, two decimal places).\\n\\nMany of your SQL SELECT statements, from SQL Server/Oracle/etc. can often be copy-paste (minus the `INSERT`) and run with minimal modification to address vendor-specific differences for things like string concatenation (SQL Server uses `+`), SQL `isnull` vs. Oracle `nvl` vs. `coalesce`, date functions etc.\\n\\nBut there are a few points of caution to look out for:\\n\\n### UPDATEs and DELETEs\\n\\nSometimes SQL-based ETL procesess use `UPDATE` or `DELETE` statements. Or you may have a series of `INSERT` statements a table with `WHERE NOT EXISTS` to prevent duplicates.\\n\\nThese cannot be copied as-is to Spark. Rather, you need to do some refactoring to work with `SELECT` (read) operations only. For example:\\n\\n- Multiple `INSERT` statements become multiple `SELECT` statements combined with `UNION`\\n- An `UPDATE` becomes an outer join and combined in the original `SELECT`\\n- A `DELETE` becomes a filter against the original `SELECT`\\n- You might need to write the original `SELECT` results to a temporary location.\\n\\n### Correlated subqueries\\n\\nSome kinds of subqueries need to be re-written. For example, Spark does not support `EXISTS`/`NOT EXISTS` subqueries. These must be rewritten as [semi-joins or anti-joins](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html). For example:\\n\\n```sql\\nselect * from table_a a where not exists(select 1 from table_b b where b.a_id=a.a_id)\\n```\\n\\nbecomes\\n\\n```sql\\nselect * from table_a a ANTI JOIN table_b on a.a_id = b.a_id\\n```\\n\\n### Implicit type conversions\\n\\nIn SQL-based ETL processes, an `INSERT` statement may do an implicit type conversion to the target column. In Spark you will have to do that conversion explicitly.\\n\\nAt the same time, Spark may do some implicit type conversion of its own on arithmetic and aggregate functions. For example, a `SUM` in Spark will promote the underlying numeric type: an `int` will become a `bigint`, a `decimal(p, s)` will become `decimal(p + 10, s)` (expand by ten digits). So if you intend to preserve the datatypes that correspond to your original database schema, you will have to cast explicitly.\\n\\nFor example:\\n\\n```scala\\nval df = spark.sql(\\"\\"\\"\\n  SELECT patient_id, patient_name, cast(count(1) as integer) visit_count\\n  FROM  patient\\n  JOIN  visit ON patient.patient_id = visit.patient_id\\n  GROUP BY patient_id, patient_name\\n\\"\\"\\")\\n```\\n\\n### Decimal truncation on division\\n\\nOn some databases (SQL Server for example), division operations on decimal types may truncate to a [specified number of decimal places](https://docs.microsoft.com/en-us/sql/t-sql/data-types/precision-scale-and-length-transact-sql?view=sql-server-ver15). Spark follows [similar rules to SQL Server](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DecimalPrecision.scala) but rounds rather than truncates.\\n\\nIf your goal is to reproduce exact results in Spark (useful to simplify validation in a technology migration), you may need to define a [user-defined function](https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html):\\n\\n```scala\\nval divide = udf((x: BigDecimal, y: BigDecimal, scale: Int) => x.divide(y, scale, RoundingMode.FLOOR))\\nspark.udf.register(\\"divide\\", divide)\\nspark.sql(\\"select divide(foo, bar, 6) as ratio from table\\")\\n```\\n\\n### Coming up\\n\\nThe next article will discuss going beyond copy-paste SQL, to refactoring and using the Spark DataFrame API directly."},{"id":"/2021/11/15/package-model-analytic-reuse","metadata":{"permalink":"/blog/2021/11/15/package-model-analytic-reuse","source":"@site/blog/2021-11-15-package-model-analytic-reuse.md","title":"Packaging models and analytics for reuse -  API vs. Inner Source.","description":"Packaging models and analytics for reuse - API vs. Inner Source.","date":"2021-11-15T00:00:00.000Z","formattedDate":"November 15, 2021","tags":[{"label":"ML-AI","permalink":"/blog/tags/ml-ai"}],"readingTime":3.68,"truncated":true,"authors":[{"name":"Bill Schneider","title":"Sr. Principal Engineer","url":"https://www.linkedin.com/in/wrschneider","imageURL":"https://github.com/wrschneider.png"}],"frontMatter":{"title":"Packaging models and analytics for reuse -  API vs. Inner Source.","author":"Bill Schneider","author_title":"Sr. Principal Engineer","author_url":"https://www.linkedin.com/in/wrschneider","author_image_url":"https://github.com/wrschneider.png","tags":["ML-AI"],"hide_table_of_contents":false},"prevItem":{"title":"Migrating ETL to Spark - Motivation and Quick Start","permalink":"/blog/2022/01/14/spark-series-part-1"},"nextItem":{"title":"Copying data between AWS and Azure","permalink":"/blog/2021/10/21/s3-to-azure-copy"}},"content":"### Packaging models and analytics for reuse - API vs. Inner Source.\\n\\n\x3c!--truncate--\x3e\\n\\nVarious teams in Optum produce models or analytics. Simplified examples of \\"models\\":\\n\\n- risk scoring for a patient - given history for a patient, how likely are they to be admitted to the hospital as an inpatient?\\n- grouping claims into an episode - what is the all-in cost for a procedure including follow-up care?\\n- calculating quality measures - in a patient population, what percentage got their flu shots this year?\\n\\nModels can be either rules-based and explicitly coded, or ML-based.\\n\\nOnce we have models like these, how can that IP be reused in a different context? The goal should be to be able to\\ncompose various individual models or analytics into new products.\\n\\nThere are a few different approaches typically used:\\n\\n**REST APIs** The team that creates a model can deploy their model as a REST API for other teams to invoke. This\\nanalytic-as-a-service approach\\nworks well for APIs that are called inside of a workflow--for example, making a decision about an individual patient\\nat point of care, or analytics that can be recomputed incrementally without full history.\\n\\nThe benefit of APIs is that the provider and consumer tech stack doesn\'t need to align, and that consumers will see changes to the model immediately when it is deployed with no action required. The big downside is latency: REST doesn\'t work well in batch workflows if processing each individual record requires an HTTPS round-trip. The other downside is operational coupling: the team that produces the model needs to not only maintain the logic of the model itself, but also is responsible for availability and performance for the REST API. Also the provider can\'t make breaking changes without an API versioning strategy, keeping the old versions of the API live until consumers can migrate.\\n\\n**Batch API** A variation of the analytic-as-a-service approach to address the HTTPS round-trip issue with batch data, for analytics that involve aggregating historical data and can\'t recompute on incremental data alone. The payload goes through a file transfer (SFTP, Azure blob storage, AWS S3 etc.) and REST endpoints are only used to mediate the process with pointers\\nto the out-of-band payload (similar to\\n[claim check pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/claim-check)). Still, you have to ship\\ndata back and forth and the file transfer time can add significant overhead to overall turnaround time. Also, you still\\nhave operational coupling and concerns over breaking changes, although this could beinsulated from the end user interactions.\\n\\n**Inner source** Instead of offering analytics as a service, you can package an analytic as an artifact that can be\\nembedded as a dependency into any other application. This could either be a JAR or DLL that gets included as a build\\ndependency, or the output of an ML framework (PyTorch, Tensorflow etc.)\\n\\nThe benefits here are: you bring the analytic to the data, so there is no\\noverhead in transferring data back and forth. The model provider has no operational responsibility or cost. Consumers\\ncan continue using old versions of artifacts until they are ready to upgrade. The downsides are: consumers have to\\ntake action to upgrade. There are tech stack dependencies -- if the analytic is a JAR and your team is standardized on\\n.NET, or the ML framework doesn\'t have an SDK for your platform, you can\'t call it in-process. Tech stack alignment is\\nless of an issue for batch process, since different stages of a batch workflow can be on different technologies as long as\\nthey share a file system.\\n\\nThere is no one right answer, and we use different approaches in different situations. To make a decision you have to ask yourself a series of questions and decide how important each one is:\\n\\n- How often does the underlying model/analytic change?\\n- As a consumer, do you want to see changes right away or do you want to be in control over moving to the new version?\\n- Is this a real-time interaction or a batch process?\\n- How big are concerns over data transfer overhead or request latency?\\n- If you need a real-time interaction, are you on compatible tech stacks?\\n\\nAlso note that _these approaches are not mutually exclusive._ If you are a team providing a model or analytic, and you\\nthink about how to package your model for reuse via inner-source, you can _also_ provide that same model as a REST API endpoint."},{"id":"/2021/10/21/s3-to-azure-copy","metadata":{"permalink":"/blog/2021/10/21/s3-to-azure-copy","source":"@site/blog/2021-10-21-s3-to-azure-copy.md","title":"Copying data between AWS and Azure","description":"Copying data between AWS and Azure","date":"2021-10-21T00:00:00.000Z","formattedDate":"October 21, 2021","tags":[{"label":"AWS Azure S3 Cloud","permalink":"/blog/tags/aws-azure-s-3-cloud"},{"label":"engineering","permalink":"/blog/tags/engineering"}],"readingTime":2.875,"truncated":true,"authors":[{"name":"Bill Schneider","title":"Sr. Principal Engineer","url":"https://github.com/wrschneider","imageURL":"https://github.com/wrschneider.png"}],"frontMatter":{"title":"Copying data between AWS and Azure","author":"Bill Schneider","author_title":"Sr. Principal Engineer","author_url":"https://github.com/wrschneider","author_image_url":"https://github.com/wrschneider.png","tags":["AWS Azure S3 Cloud","engineering"],"hide_table_of_contents":false},"prevItem":{"title":"Packaging models and analytics for reuse -  API vs. Inner Source.","permalink":"/blog/2021/11/15/package-model-analytic-reuse"},"nextItem":{"title":"Knative Channel Sans Kafka Admin Rights","permalink":"/blog/2021/09/27/kn-kafka-topic-channel"}},"content":"### Copying data between AWS and Azure\\n\\n\x3c!--truncate--\x3e\\nMany Optum teams are working in Azure, although some are also working in AWS.  So sometimes we have to transfer files between\\nAWS and Azure, for batch-processing pipelines that have components in both clouds.\\n\\nThe easiest way to move files between clouds is through their respective object storage APIs: Azure Blob Storage or \\nAWS S3.  Transfers are over port 443 with public-facing endpoints, and authentication/authorization is handled like\\nall other native cloud services.\\n\\nThere are good open-source tools for AWS/Azure file transfers, including\\n[`azcopy`](https://github.com/Azure/azure-storage-azcopy) and \\n[`rclone`](https://github.com/rclone/rclone).  Both are command-line tools that can be executed from automated \\nprocesses.  `azcopy` is specialized to work with Azure, but lacks the ability to move data from Azure to \\nAWS S3.  `rclone` is general purpose any-to-any transfers, but may not be as optimized for uploads to Azure.\\n\\nThese tools can easily be installed in Docker containers.  If you are running on AWS, you might want to execute\\na transfer to/from Azure as a serverless ECS Fargate task.  When you run the task, you would pass the specific\\ncommand line parameters in the `containerOverrides`.  This is all standard; the only tricky piece is \\nhow to manage credentials for both AWS and Azure.  Secrets that can be passed through environment variables can be\\n[stored in AWS Parameter Store or Secrets Manager and retrieved with `secrets` in the container definition](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html).  Others require wrapper scripts to perform\\nadditional tasks before running azcopy or rclone.\\n\\n* azcopy \\n  * AWS credentials: for some reason, [azcopy doesn\'t use native AWS profiles](https://github.com/Azure/azure-storage-azcopy/issues/1341) and only takes AWS credentials from environment variables.  So you need a wrapper script to set these up.\\n  * Azure SAS tokens: azcopy expects these on the command line, so it\'s a good idea for a wrapper script to append this to the URL\\n  * Azure service principal: secret can be passed through `AZCOPY_SPA_CLIENT_SECRET` environment variable.\\n\\n* Rclone \\n  * AWS credentials: uses the AWS SDK and will pick up AWS credentials from the container\'s execution role.\\n  * Azure SAS tokens: SAS URL can be passed via `RCLONE_AZUREBLOB_SAS_URL` environment variable as container secret.\\n  * Azure service principal: needs to be stored as JSON in a file.  Your container needs a wrapper script that takes \\n  the actual secret from the injected environment variable, and writes it to a file.\\n\\nExamples of wrapper script for `azcopy` to set up AWS credentials from the ECS metadata endpoint:\\n\\n```sh\\nresponse=$(wget -O - 169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI)\\nexport AWS_ACCESS_KEY_ID=$(echo \\"$response\\" | jq -r \'.AccessKeyId\')\\nexport AWS_SECRET_ACCESS_KEY=$(echo \\"$response\\" | jq -r \'.SecretAccessKey\')\\nexport AWS_SESSION_TOKEN=$(echo \\"$response\\" | jq -r \'.Token\')\\nazcopy $@\\n```\\n\\nIf you are copying to Azure and need to keep the SAS token secret, you can append modify the above script to\\nappend it to the URL and then omit the SAS token from the Azure blob storage URL passed in `containerOverrides`:\\n\\n```sh\\nazcopy $@$AZURE_SECRET\\n```\\n\\nNote that this only works because azcopy can only be used to copy *from* S3 *to* Azure, not the other way around, so\\nwe know the Azure URL will always be the final argument on the command line.\\n\\nFor rclone, this is a wrapper script to handle the service principal JSON:\\n\\n```sh\\n# assume AZURE_SECRET is configured with secret in container definition\\necho \\"$AZURE_SECRET\\" > /tmp/secret.json\\nexport RCLONE_AZUREBLOB_SERVICE_PRINCIPAL_FILE=/tmp/secret.json\\nrclone $@\\n```\\n\\nAll of the above will allow you to do your Azure/AWS file transfers in a serverless Fargate task, that can be\\nintegrated with whatever other pipeline workflow you have."},{"id":"/2021/09/27/kn-kafka-topic-channel","metadata":{"permalink":"/blog/2021/09/27/kn-kafka-topic-channel","source":"@site/blog/2021-09-27-kn-kafka-topic-channel.md","title":"Knative Channel Sans Kafka Admin Rights","description":"Knative","date":"2021-09-27T00:00:00.000Z","formattedDate":"September 27, 2021","tags":[{"label":"engineering","permalink":"/blog/tags/engineering"},{"label":"knative","permalink":"/blog/tags/knative"},{"label":"channel","permalink":"/blog/tags/channel"},{"label":"broker","permalink":"/blog/tags/broker"},{"label":"kafka","permalink":"/blog/tags/kafka"},{"label":"message","permalink":"/blog/tags/message"},{"label":"events","permalink":"/blog/tags/events"},{"label":"cloudevents","permalink":"/blog/tags/cloudevents"}],"readingTime":2.035,"truncated":true,"authors":[{"name":"Murugappan Chetty","title":"Principal Software Engineer","url":"https://github.com/itsmurugappan","imageURL":"https://github.com/itsmurugappan.png"}],"frontMatter":{"title":"Knative Channel Sans Kafka Admin Rights","author":"Murugappan Chetty","author_title":"Principal Software Engineer","author_url":"https://github.com/itsmurugappan","author_image_url":"https://github.com/itsmurugappan.png","tags":["engineering","knative","channel","broker","kafka","message","events","cloudevents"],"hide_table_of_contents":false},"prevItem":{"title":"Copying data between AWS and Azure","permalink":"/blog/2021/10/21/s3-to-azure-copy"},"nextItem":{"title":"AI Racing League","permalink":"/blog/2021/09/08/AI Racing League"}},"content":"### Knative\\n\\n\x3c!--truncate--\x3e\\n\\nKnative enables you to run a serverless platform on your own terms. It has 2 independent components, Serving and Eventing. Serving runs your application as a serverless container in a kubernetes cluster, while eventing provides the tooling to source and orchestate events to your application.\\n\\nOnce you get past the initial attraction of scale to 0 that serving provides, you will quickly take notice of knative eventing capabilities mentioned below\\n\\n### Event Source\\n\\nComponent provided by Knative Eventing to source events from a actual event producer like kafka/rabbit mq/github etc. and deliver it to a addressable resource (any resource which has a uri, can be knative service/kubernetes service or just an uri). There is an event source for almost all commonly used event producers. Either its community maintained or custom implementation.\\n\\n### Brokers and Triggers\\n\\nSource delivers events 1-1. A fan out model with filters would be great for orchestrating events. Thats what Brokers and Triggers provide. Broker as the name suggest is the event ingress and hub, triggers route the events based on filters.\\n\\nFor brokers, triggers and sources to work in harmony they need to speak the common language and that is provided by [Cloud Events](https://cloudevents.io).\\n\\nBelow image shows how the events are sourced and routed to different knative and kubernetes service.\\n\\n![](./images/brokertrigger.jpg)\\n\\n### What\'s a Knative Channel\\n\\nBrokers, as pointed above is the events hub, so it needs a state store, which is provided by `Channels`. By default Brokers use InMemory Channels. In a production environment, you would need a more robust store, for which Knative provides `KafkaChannel` and `NatsChannel`. Its worth mentioning here that there are broker implementations (kafka broker and rabbit mq broker) which dont require channels.\\n\\n### Kafka Topic Channel\\n\\nThe kafka channel or kafka broker would need admin rights as topics are created for each resource. This would require the Knative operator to main a kafka cluster, which might be cumbersome. Hence we implemented a custom knative channel based on `kafka topic`.\\n\\nKafka Topic Channel or ktc conforms to the knative spec, hence brokers and triggers would work as is. It is a single tenant solution, each Broker resource would require separate kafka topic and subsequent kafka topic channel. The channel instance would be created in the user namespace.\\n\\n[Details with examples](https://github.com/Optum/kafka-topic-channel)\\n\\nBring your own kafka topic, get a knative channel!\\n\\n#### Related Resources\\n\\n- [Kubecon demo on cloud events, knative brokers and triggers](https://www.youtube.com/watch?v=gXuW9mvj6xM&list=PLnPNqTSUj2hKH5W7GWOZ-mzcw4r3O4bHj&index=2&t=14s)\\n- [Event Sources](https://itsmurugappan.medium.com/writing-custom-knative-eventing-sources-92f6904131ad)"},{"id":"/2021/09/08/AI Racing League","metadata":{"permalink":"/blog/2021/09/08/AI Racing League","source":"@site/blog/2021-09-08-AI Racing League.md","title":"AI Racing League","description":"AI Racing League","date":"2021-09-08T00:00:00.000Z","formattedDate":"September 8, 2021","tags":[{"label":"ML-AI","permalink":"/blog/tags/ml-ai"}],"readingTime":2.47,"truncated":true,"authors":[{"name":"Dan McCreary","url":"https://github.com/dmccreary","imageURL":"https://github.com/dmccreary.png"}],"frontMatter":{"title":"AI Racing League","author":"Dan McCreary","author_url":"https://github.com/dmccreary","author_image_url":"https://github.com/dmccreary.png","tags":["ML-AI"],"description":"AI Racing League","hide_table_of_contents":false},"prevItem":{"title":"Knative Channel Sans Kafka Admin Rights","permalink":"/blog/2021/09/27/kn-kafka-topic-channel"},"nextItem":{"title":"Our UHG Open Source Program Office (OSPO)","permalink":"/blog/2021/08/24/OSPO culture"}},"content":"### AI Racing League\\n\\n\x3c!--truncate--\x3e\\n\\nIn March of 2019, I approached our executive leadership team in Optum Technology. We discussed the importance of making artificial intelligence (AI) a core part of the company culture. We brainstormed with stories of other companies\' efforts. Then the conversation turned to work I had been doing outside of Optum teaching kids about AI. My partners and I have built coding clubs, such as CoderDojo Twin Cities. A lot of companies use an Open Source system called DonkeyCar to teach AI. We discussed how this might be a vehicle for our technical talent to contribute back by mentoring in communities.\\n\\nThese racing leagues are a great, innovative way to bring STEM mentorship to the next generation of healthcare leaders. The events are filled with fun and laughter. Clearly, these people and kids are having fun learning AI! Our Optum leaders were supportive - we agreed that Optum wants to invest in STEM mentorship. But how would we get the ball rolling to make this happen?\\n\\nMy leadership team guided me to write a proposal. Then they helped me identify a business sponsor in our internal training division. I also met people who were passionate about education and curriculum development, a topic I am also very interested in. We ran our first \\"AI Racing League\\" event on Aug. 6th 2019.\\n\\nWe learned a LOT about introducing AI/ML to non-technical staff during this first session. First of all, it was REALLY fun! We also learned how to break all the content and we designed a topic dependency graph of all the topics that we wanted to cover: RC car hardware, Nivida Nano hardware, calibration, cameras, UNIX shell, Python, data analysis, Jupyter Notebooks, computer vision, GPUs, training, model management, model evaluation.\\n\\nFor each of these concepts, we gathered documentation and resources and created breakout tables that our participants visited for the first part of the events. Then we broke down into teams and each team got a car they had to train. We built GPU servers that could build models in under five minutes.\\n\\nThe result? Many of our participants told us this was the \u201cfunnest\u201d class they have EVER attended as part of our internal training. Since that event, we have had about a dozen follow-up events. As soon as we can gather again in person, we hope to continue live AI Racing League events. In the meantime, we\'re revising our materials with a focus on work on growth and multi-modal expansion. We are preparing for another great in-person racing season. We would love to collaborate with others that might find ways to host our curricula online and convert our materials for use primarily online. We now have many trained mentors and hundreds of alumni of the classes that are strongly recommending these courses to their colleagues. Reach out to me to connect!\\n\\nWe\'re happy to discuss how to bring this STEM mentorship program in ML/AI into your school or club!"},{"id":"/2021/08/24/OSPO culture","metadata":{"permalink":"/blog/2021/08/24/OSPO culture","source":"@site/blog/2021-08-24-OSPO culture.md","title":"Our UHG Open Source Program Office (OSPO)","description":"This is my first post on Docusaurus 2.","date":"2021-08-24T00:00:00.000Z","formattedDate":"August 24, 2021","tags":[{"label":"culture","permalink":"/blog/tags/culture"},{"label":"OSPO","permalink":"/blog/tags/ospo"}],"readingTime":2.585,"truncated":true,"authors":[{"name":"DB Babjack","url":"https://github.com/dbabjaxy","imageURL":"https://github.com/dbabjaxy.png"}],"frontMatter":{"title":"Our UHG Open Source Program Office (OSPO)","author":"DB Babjack","author_url":"https://github.com/dbabjaxy","author_image_url":"https://github.com/dbabjaxy.png","tags":["culture","OSPO"],"description":"This is my first post on Docusaurus 2.","hide_table_of_contents":false},"prevItem":{"title":"AI Racing League","permalink":"/blog/2021/09/08/AI Racing League"}},"content":"### What is an OSPO?\\n\\n\x3c!--truncate--\x3e\\n\\nToday, we solidify our definition of what the United Healthgroup open source program office, or UHG OSPO, does. We are the center of competency for this organization\'s open source operations and structure. The Open Source Program Office (OSPO) owns and promotes the UHG open source strategy. After 7 years of service, we continue to focus on training and supporting developers in the open source domain. Our operations include setting code use, distribution, and selection. We train developers in methods of open contribution with support from Optum Technology University.\\n\\nCompanies create OSPOs to manage their relationships with the open source ecosystems they depend upon. It collaborates with engineering, legal, security, invention, product, technology, scientific, and strategic Open Source Software (OSS) communities to verify or adapt our strategy to implement OSS Governance as an enabler for employees to shape and influence technology needed to solve tomorrow\u2019s problems. This provides the unique opportunity to stoke innovation, realize savings and encourage an engineering mindset, while supporting the enterprise\u2019s social responsibility initiatives.\\n\\nThe Open Source Program Office (OSPO) owns and promotes the UHG open source strategy. It collaborates with engineering, legal, security, invention, product, technology, scientific, and strategic Open Source Software (OSS) communities to verify or adapt our strategy to implement OSS Governance as an enabler for employees to shape and influence technology needed to solve tomorrow\u2019s problems. This provides the unique opportunity to stoke innovation, realize savings and encourage an engineering mindset, while supporting the enterprise\u2019s social responsibility initiatives.\\n\\nAt their crux, OSPOs manage the inbound and outbound flow of open source projects in their company\'s ecosystem. The company\'s software health and sustainability are partly dependent on the Open Source Software ecosystem surrounding these activities. In one role, OSPOs are rangers of the organization\'s OSS ecosystem. In another, OSPOs are promoters of action in an open source community. OSPOs are inventors sitting at the intersection between technology and social responsibility.\\n\\nAt UHG, our OSPO exists to accomplish a set of primary objectives.\\n\\n### Primary Objectives\\n\\n- Continually Evolve OSS Policy and Process to Support Engineering and Business Communities\\n- Drive Increased OSS Policy and Process Knowledge Across the Organization\\n- Support Increased OSS Policy Compliance to Reduce Risk\\n- Reduce Engineering Friction across the Organization\\n- Increase Speed to Minimal Viable Products (MVPs) through Open Source Code Use and ReUse\\n- Reduce external open source dependencies\\n- Continually Educate Stakeholders on OSPO Capabilities and Value\\n- Contribute to and Balance the Portfolio of Intellectual Property and Open Source Contribution\\n- Elevate the Technical Eminence of the Healthcare Technology Industry\\n\\nAs open-source leaders, we positively disrupt the healthcare and technology industries. We are a vibrant, celebrated, and supported open-source community. We proudly deliver innovative solutions to the world\'s toughest healthcare problems. Join us!\\n\\n### Resources and References\\n\\nOptum Open Source. https://optum.github.io/\\n\\nTODO. https://todogroup.org/blog/ospo-definition/\\n\\nCHAOSS. https://chaoss.community/\\n\\n### Get Involved\\n\\nEmail opensource@optum.com\\n\\n---\\n\\n#### Disclaimer\\n\\nThe information presented on this site about systems, technologies, and services are the express views of the authors and do not constitute endorsement, recommendation, or favoring by United Health group and its subsidiaries."}]}')}}]);